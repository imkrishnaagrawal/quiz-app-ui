[
    {
        "questionNo": 0,
        "questionText": "A company has an application hosted in an Auto Scaling group of Amazon EC2 instances across multiple Availability Zones behind an Application Load Balancer. There are several occasions where some instances are automatically terminated after failing the HTTPS health checks in the ALB and then purges all the ephemeral logs stored in the instance. A Solutions Architect must implement a solution that collects all of the application and server logs effectively. She should be able to perform a root cause analysis based on the logs, even if the Auto Scaling group immediately terminated the instance.\n\nWhat is the EASIEST way for the Architect to automate the log collection from the Amazon EC2 instances?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nAdd a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state to delay the termination of the unhealthy Amazon EC2 instances. Configure a CloudWatch Events rule for the EC2 Instance Terminate Successful Auto Scaling Event with an associated Lambda function. Set up the AWS Systems Manager Run Command service to run a script that collects and uploads the application logs from the instance to a CloudWatch Logs group. Resume the instance termination once all the logs are sent."
            },
            {
                "isCorrect": false,
                "text": "​\n\nAdd a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state to delay the termination of the unhealthy Amazon EC2 instances. Set up AWS Step Functions to collect the application logs and send them to a CloudWatch Log group. Configure the solution to resume the instance termination as soon as all the logs were successfully sent to CloudWatch Logs."
            },
            {
                "isCorrect": true,
                "text": "​\n\nAdd a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state to delay the termination of unhealthy Amazon EC2 instances. Configure a CloudWatch Events rule for the EC2 Instance-terminate Lifecycle Action Auto Scaling Event with an associated Lambda function. Trigger the CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAdd a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Pending:Wait state to delay the termination of the unhealthy Amazon EC2 instances. Configure a CloudWatch Events rule for the EC2 Instance-terminate Lifecycle Action Auto Scaling Event with an associated Lambda function. Set up an AWS Systems Manager Automation script that collects and uploads the application logs from the instance to a CloudWatch Logs group. Configure the solution to only resume the instance termination once all the logs were successfully sent."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The EC2 instances in an Auto Scaling group have a path, or lifecycle, that differs from that of other EC2 instances. The lifecycle starts when the Auto Scaling group launches an instance and puts it into service. The lifecycle ends when you terminate the instance, or the Auto Scaling group takes the instance out of service and terminates it.</p><p>You can add a lifecycle hook to your Auto Scaling group so that you can perform custom actions when instances launch or terminate.</p><p>When Amazon EC2 Auto Scaling responds to a scale out event, it launches one or more instances. These instances start in the <code>Pending</code> state. If you added an <code>autoscaling:EC2_INSTANCE_LAUNCHING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Pending</code> state to the <code>Pending:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Pending:Proceed</code> state. When the instances are fully configured, they are attached to the Auto Scaling group and they enter the <code>InService</code> state.</p><p>When Amazon EC2 Auto Scaling responds to a scale in event, it terminates one or more instances. These instances are detached from the Auto Scaling group and enter the <code>Terminating</code> state. If you added an <code>autoscaling:EC2_INSTANCE_TERMINATING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Terminating:Proceed</code> state. When the instances are fully terminated, they enter the <code>Terminated</code> state.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/auto_scaling_lifecycle.png\"></p><p><br></p><p>Using CloudWatch agent is the most suitable tool to use to collect the logs. The unified CloudWatch agent enables you to do the following:</p><p>- Collect more system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html\">Metrics Collected by the CloudWatch Agent</a>.</p><p>- Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p><p>- Retrieve custom metrics from your applications or services using the <code>StatsD</code> and <code>collectd</code> protocols. <code>StatsD</code> is supported on both Linux servers and servers running Windows Server. <code>collectd</code> is supported only on Linux servers.</p><p>- Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p><p><br></p><p><img src=\"https://udemy-images.s3.amazonaws.com/redactor/raw/2020-02-15_12-00-37-694540fbb799233b0fcd7240558c6c85.png\"></p><p><br></p><p>You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is <code>CWAgent</code>, although you can specify a different namespace when you configure the agent.</p><p>Hence, the correct answer is: <strong><em>Add a lifecycle hook to your Auto Scaling group to move instances in the </em></strong><code><strong><em>Terminating</em></strong></code><strong><em> state to the </em></strong><code><strong><em>Terminating:Wait</em></strong></code><strong><em> state to delay the termination of unhealthy Amazon EC2 instances. Configure a CloudWatch Events rule for the </em></strong><code><strong><em>EC2 Instance-terminate Lifecycle Action</em></strong></code><strong><em> Auto Scaling Event with an associated Lambda function. Trigger the CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.</em></strong></p><p>The option that says:<strong><em> Add a lifecycle hook to your Auto Scaling group to move instances in the </em></strong><code><strong><em>Terminating</em></strong></code><strong><em> state to the </em></strong><code><strong><em>Pending:Wait</em></strong></code><strong><em> state to delay the termination of the unhealthy Amazon EC2 instances. Configure a CloudWatch Events rule for the </em></strong><code><strong><em>EC2 Instance-terminate Lifecycle Action</em></strong></code><strong><em> Auto Scaling Event with an associated Lambda function. Set up an AWS Systems Manager Automation script that collects and uploads the application logs from the instance to a CloudWatch Logs group. Configure the solution to only resume the instance termination once all the logs were successfully sent</em></strong> is incorrect because the <code><strong><em>Pending:Wait</em></strong></code><strong><em> </em></strong>state refers to the scale-out action in Amazon EC2 Auto Scaling and not for scale-in or for terminating the instances.</p><p>The option that says:<strong><em> Add a lifecycle hook to your Auto Scaling group to move instances in the </em></strong><code><strong><em>Terminating</em></strong></code><strong><em> state to the </em></strong><code><strong><em>Terminating:Wait</em></strong></code><strong><em> state to delay the termination of the unhealthy Amazon EC2 instances. Set up AWS Step Functions to collect the application logs and send them to a CloudWatch Log group. Configure the solution to resume the instance termination as soon as all the logs were successfully sent to CloudWatch Logs</em></strong> is incorrect because using AWS Step Functions is inappropriate in collecting the logs from your EC2 instances. You should use a CloudWatch agent instead.</p><p>The option that says: <strong><em>Add a lifecycle hook to your Auto Scaling group to move instances in the </em></strong><code><strong><em>Terminating</em></strong></code><strong><em> state to the </em></strong><code><strong><em>Terminating:Wait</em></strong></code><strong><em> state to delay the termination of the unhealthy Amazon EC2 instances. Configure a CloudWatch Events rule for the </em></strong><code><strong><em>EC2 Instance Terminate Successful</em></strong></code><strong><em> Auto Scaling Event with an associated Lambda function. Set up the AWS Systems Manager Run Command service to run a script that collects and uploads the application logs from the instance to a CloudWatch Logs group. Resume the instance termination once all the logs are sent</em></strong> is incorrect because although this solution could work, it entails a lot of effort to write a custom script that the AWS Systems Manager Run Command will run. Remember that the scenario asks for a solution that you can implement with the least amount of effort. This solution can be simplified by automatically uploading the logs using a CloudWatch Agent. You have to use the <code>EC2 Instance-terminate Lifecycle Action</code> event instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/cloud-watch-events.html#terminate-successful\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/cloud-watch-events.html#terminate-successful</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/\">https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/</a></p></div>"
    },
    {
        "questionNo": 1,
        "questionText": "An On-Demand EC2 instance is launched into a VPC subnet with the Network ACL configured to allow all inbound traffic and deny all outbound traffic. The instance’s security group has an inbound rule to allow SSH from any IP address and does not have any outbound rules. \n\n\nIn this scenario, what are the changes needed to allow SSH connection to the instance?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nThe network ACL needs to be modified to allow outbound traffic.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nThe outbound security group needs to be modified to allow outbound traffic."
            },
            {
                "isCorrect": false,
                "text": "​\nNo action needed. It can already be accessed from any IP address using SSH."
            },
            {
                "isCorrect": false,
                "text": "​\nBoth the outbound security group and outbound network ACL need to be modified to allow outbound traffic."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In order for you to establish an SSH connection from your home computer to your EC2 instance, you need to do the following:</p><p>- On the Security Group, add an Inbound Rule to allow SSH traffic to your EC2 instance.</p><p>- On the NACL, add both an Inbound and Outbound Rule to allow SSH traffic to your EC2 instance.</p><p><br></p><p>The reason why you have to add both Inbound and Outbound SSH rule is due to the fact that Network ACLs are stateless which means that responses to allow inbound traffic are subject to the rules for outbound traffic (and vice versa). In other words, if you only enabled an Inbound rule in NACL, the traffic can only go in but the SSH response will not go out since there is no Outbound rule.</p><p>Security groups are stateful which means that if an incoming request is granted, then the outgoing traffic will be automatically granted as well, regardless of the outbound rules.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html</a></p></div>"
    },
    {
        "questionNo": 2,
        "questionText": "A client is hosting their company website on a cluster of web servers that are behind a public-facing load balancer. The client also uses Amazon Route 53 to manage their public DNS.   \n\nHow should the client configure the DNS zone apex record to point to the load balancer?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nCreate an A record pointing to the IP address of the load balancer."
            },
            {
                "isCorrect": true,
                "text": "​\nCreate an A record aliased to the load balancer DNS name.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nCreate a CNAME record pointing to the load balancer DNS name."
            },
            {
                "isCorrect": false,
                "text": "​\nCreate an alias for CNAME record to the load balancer DNS name."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Route 53's DNS implementation connects user requests to infrastructure running inside (and outside) of Amazon Web Services (AWS). For example, if you have multiple web servers running on EC2 instances behind an Elastic Load Balancing load balancer, Route 53 will route all traffic addressed to your website (e.g. <code>www.tutorialsdojo.com</code>) to the load balancer DNS name (e.g. <code>elbtutorialsdojo123.elb.amazonaws.com</code>).</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/govcloud-us/latest/ug-west/images/r53-cf-elb.png\"></p><p><br></p><p>Additionally, Route 53 supports the alias resource record set, which lets you map your <strong>zone apex (</strong>e.g. <code>tutorialsdojo.com</code>) DNS name to your load balancer DNS name. IP addresses associated with Elastic Load Balancing can change at any time due to scaling or software updates. Route 53 responds to each request for an Alias resource record set with one IP address for the load balancer.</p><p><strong>Creating an A record pointing to the IP address of the load balancer</strong> is incorrect. You should be using an Alias record pointing to the DNS name of the load balancer since the IP address of the load balancer can change at any time.</p><p><strong>Creating a CNAME record pointing to the load balancer DNS name</strong> and <strong>creating an alias for CNAME record to the load balancer DNS name</strong> are incorrect because CNAME records cannot be created for your <strong>zone</strong> apex. You should create an alias record at the top node of a DNS namespace which is also known as the <em>zone apex</em>. For example, if you register the DNS name tutorialsdojo.com, the zone apex is tutorialsdojo.com. You can't create a CNAME record directly for tutorialsdojo.com, but you can create an alias record for tutorialsdojo.com that routes traffic to <strong>www</strong>.tutorialsdojo.com.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"http://docs.aws.amazon.com/govcloud-us/latest/UserGuide/setting-up-route53-zoneapex-elb.html\">http://docs.aws.amazon.com/govcloud-us/latest/UserGuide/setting-up-route53-zoneapex-elb.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</a></p></div>"
    },
    {
        "questionNo": 3,
        "questionText": "A data analytics company keeps a massive volume of data that they store in their on-premises data center. To scale their storage systems, they are looking for cloud-backed storage volumes that they can mount using Internet Small Computer System Interface (iSCSI) devices from their on-premises application servers. They have an on-site data analytics application that frequently accesses the latest data subsets locally while the older data are rarely accessed. You are required to minimize the need to scale the on-premises storage infrastructure while still providing their web application with low-latency access to the data.\n\nWhich type of AWS Storage Gateway service will you use to meet the above requirements?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nVolume Gateway in cached mode\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nVolume Gateway in stored mode"
            },
            {
                "isCorrect": false,
                "text": "​\n\nTape Gateway"
            },
            {
                "isCorrect": false,
                "text": "​\n\nFile Gateway"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the technology company is looking for a storage service that will enable their analytics application to frequently access the latest data subsets and not the entire data set as it was mentioned that the old data are rarely being used. This requirement can be fulfilled by setting up a Cached Volume Gateway in AWS Storage Gateway.</p><p>By using cached volumes, you can use Amazon S3 as your primary data storage, while retaining frequently accessed data locally in your storage gateway. Cached volumes minimize the need to scale your on-premises storage infrastructure, while still providing your applications with low-latency access to frequently accessed data. You can create storage volumes up to 32 TiB in size and afterward, attach these volumes as iSCSI devices to your on-premises application servers. When you write to these volumes, your gateway stores the data in Amazon S3. It retains the recently read data in your on-premises storage gateway's cache and uploads buffer storage.</p><p>Cached volumes can range from 1 GiB to 32 TiB in size and must be rounded to the nearest GiB. Each gateway configured for cached volumes can support up to 32 volumes for a total maximum storage volume of 1,024 TiB (1 PiB).</p><p>In the cached volumes solution, AWS Storage Gateway stores all your on-premises application data in a storage volume in Amazon S3. Hence, the correct answer is: <strong>Volume Gateway in cached mode</strong>.</p><p><strong>Volume Gateway in stored mode</strong> is incorrect because the requirement is to provide low latency access to the frequently accessed data subsets locally. Stored Volumes are used if you need low-latency access to your entire dataset.</p><p><strong>Tape Gateway</strong> is incorrect because this is just a cost-effective, durable, long-term offsite alternative for data archiving, which is not needed in this scenario.</p><p><strong>File Gateway</strong> is incorrect because this does not provide you the required low-latency access to the frequently accessed data that the on-site analytics application needs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#volume-gateway-concepts\">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#volume-gateway-concepts</a></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Associate Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c02/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c02/</a></p></div>"
    },
    {
        "questionNo": 4,
        "questionText": "An application is hosted in an Auto Scaling group of EC2 instances. To improve the monitoring process, you have to configure the current capacity to increase or decrease based on a set of scaling adjustments. This should be done by specifying the scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process.\n\nWhich of the following is the most suitable type of scaling policy that you should use?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nStep scaling\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nTarget tracking scaling"
            },
            {
                "isCorrect": false,
                "text": "​\n\nScheduled Scaling"
            },
            {
                "isCorrect": false,
                "text": "​\n\nSimple scaling"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>With step scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process as well as define how your scalable target should be scaled when a threshold is in breach for a specified number of evaluation periods. Step scaling policies increase or decrease the current capacity of a scalable target based on a set of scaling adjustments, known as step adjustments. The adjustments vary based on the size of the alarm breach. After a scaling activity is started, the policy continues to respond to additional alarms, even while a scaling activity is in progress. Therefore, all alarms that are breached are evaluated by Application Auto Scaling as it receives the alarm messages.</p><p>When you configure dynamic scaling, you must define how to scale in response to changing demand. For example, you have a web application that currently runs on two instances and you want the CPU utilization of the Auto Scaling group to stay at around 50 percent when the load on the application changes. This gives you extra capacity to handle traffic spikes without maintaining an excessive amount of idle resources. You can configure your Auto Scaling group to scale automatically to meet this need. The policy type determines how the scaling action is performed.</p><p><br></p><p><img src=\"https://media.amazonwebservices.com/blog/2015/as_create_stepped_group_5.png\"></p><p><br></p><p>Amazon EC2 Auto Scaling supports the following types of scaling policies:</p><p><strong>Target tracking scaling - </strong>Increase or decrease the current capacity of the group based on a target value for a specific metric. This is similar to the way that your thermostat maintains the temperature of your home – you select a temperature and the thermostat does the rest.</p><p><strong>Step scaling - </strong>Increase or decrease the current capacity of the group based on a set of scaling adjustments, known as <em>step adjustments</em>, that vary based on the size of the alarm breach.</p><p><strong>Simple scaling - </strong>Increase or decrease the current capacity of the group based on a single scaling adjustment.</p><p>If you are scaling based on a utilization metric that increases or decreases proportionally to the number of instances in an Auto Scaling group, then it is recommended that you use target tracking scaling policies. Otherwise, it is better to use step scaling policies instead.</p><p>Hence, the correct answer in this scenario is <strong>Step Scaling</strong>.</p><p><strong>Target tracking scaling</strong> is incorrect because the target tracking scaling policy increases or decreases the current capacity of the group based on a <strong>target value for a specific metric</strong>, instead of a set of scaling adjustments.</p><p><strong>Simple scaling</strong> is incorrect because the simple scaling policy increases or decreases the current capacity of the group based on a <strong>single</strong> scaling adjustment, instead of a set of scaling adjustments.</p><p><strong>Scheduled Scaling</strong> is incorrect because the scheduled scaling policy is based on a schedule that allows you to set your own scaling schedule for <strong>predictable</strong> load changes. This is not considered as one of the types of dynamic scaling.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html\">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html</a></p></div>"
    },
    {
        "questionNo": 5,
        "questionText": "You are working as an IT Consultant for a large media company where you are tasked to design a web application that stores static assets in an Amazon Simple Storage Service (S3) bucket. You expect this S3 bucket to immediately receive over 2000 PUT requests and 3500 GET requests per second at peak hour.\n\nWhat should you do to ensure optimal performance?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nDo nothing. Amazon S3 will automatically manage performance at this scale.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse a predictable naming scheme in the key names such as sequential numbers or date time sequences."
            },
            {
                "isCorrect": false,
                "text": "​\n\nAdd a random prefix to the key names."
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse Byte-Range Fetches to retrieve multiple ranges of an object data per GET request."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Amazon S3 now provides increased performance to support at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data, which can save significant processing time for no additional charge. Each S3 prefix can support these request rates, making it simple to increase performance significantly.</p><p>Applications running on Amazon S3 today will enjoy this performance improvement with no changes, and customers building new applications on S3 do not have to make any application customizations to achieve this performance. Amazon S3's support for parallel requests means you can scale your S3 performance by the factor of your compute cluster, without making any customizations to your application. Performance scales per prefix, so you can use as many prefixes as you need in parallel to achieve the required throughput. There are no limits to the number of prefixes.</p><p><br></p><p><img src=\"https://d1.awsstatic.com/cloud-storage/S3BatchOperationsHowitworks.99e5996f5d9dc4648160489412900a2106d1673c.png\"></p><p><br></p><p>This S3 request rate performance increase removes any previous guidance to randomize object prefixes to achieve faster performance. That means you can now use logical or sequential naming patterns in S3 object naming without any performance implications. This improvement is now available in all AWS Regions.</p><p><strong>Using Byte-Range Fetches to retrieve multiple ranges of an object data per GET request</strong> is incorrect because although a Byte-Range Fetch helps you achieve higher aggregate throughput, Amazon S3 does <strong>not</strong> support retrieving multiple ranges of data per GET request. Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted.</p><p><strong>Adding a random prefix to the key names</strong> is incorrect. Adding a random prefix is not required in this scenario because S3 can now scale automatically to adjust perfomance. You do not need to add a random prefix anymore for this purpose since S3 has increased performance to support at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data, which covers the workload in the scenario.</p><p><strong>Using a predictable naming scheme in the key names such as sequential numbers or date time sequences</strong> is incorrect because Amazon S3 already maintains an index of object key names in each AWS region. S3 stores key names in alphabetical order. The key name dictates which partition the key is stored in. Using a sequential prefix increases the likelihood that Amazon S3 will target a specific partition for a large number of your keys, overwhelming the I/O capacity of the partition.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/AmazonS3BestPractices.pdf\">https://d1.awsstatic.com/whitepapers/AmazonS3BestPractices.pdf</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/GettingObjectsUsingAPIs.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/GettingObjectsUsingAPIs.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</a></p></div>"
    },
    {
        "questionNo": 6,
        "questionText": "You are consulted by a multimedia company that needs to deploy web services to an AWS region which they have never used before. The company currently has an IAM role for their Amazon EC2 instance which permits the instance to access Amazon DynamoDB. They want their EC2 instances in the new region to have the exact same privileges.   \n\nWhat should you do to accomplish this?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\nAssign the existing IAM role to instances in the new region.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nCreate an Amazon Machine Image (AMI) of the instance and copy it to the new region."
            },
            {
                "isCorrect": false,
                "text": "​\nIn the new Region, create a new IAM role and associated policies then assign it to the new instance."
            },
            {
                "isCorrect": false,
                "text": "​\nDuplicate the IAM role and associated policies to the new region and attach it to the instances."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the company has an existing IAM role hence you don’t need to create a new one. IAM roles are global service that are available to all regions hence, all you have to do is assign the existing IAM role to the instance in the new region.</p><p>The option that says: <strong>In the new Region, create a new IAM role and associated policies then assign it to the new instance</strong> is incorrect because you don't need to create another IAM role - there is already an existing one.</p><p><strong>Duplicating the IAM role and associated policies to the new region and attaching it to the instances</strong> is incorrect as you don't need duplicate IAM roles for each region. One IAM role suffices for the instances on two regions.</p><p><strong>Creating an Amazon Machine Image (AMI) of the instance and copying it to the new region</strong> is incorrect because creating an AMI image does not affect the IAM role of the instance.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</a></p></div>"
    },
    {
        "questionNo": 7,
        "questionText": "Your manager instructed you to use Route 53 instead of an ELB to load balance the incoming request to your web application. The system is deployed to two EC2 instances to which the traffic needs to be  distributed to. You want to set a specific percentage of traffic to go to each instance.   \n\nWhich routing policy would you use?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\nWeighted\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nFailover"
            },
            {
                "isCorrect": false,
                "text": "​\nLatency"
            },
            {
                "isCorrect": false,
                "text": "​\nGeolocation"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes including load balancing and testing new versions of software. You can set a specific percentage of how much traffic will be allocated to the resource by specifying the weights.</p> <p>For example, if you want to send a tiny portion of your traffic to one resource and the rest to another resource, you might specify weights of 1 and 255. The resource with a weight of 1 gets 1/256th of the traffic (1/1+255), and the other resource gets 255/256ths (255/1+255).</p> <p>You can gradually change the balance by changing the weights. If you want to stop sending traffic to a resource, you can change the weight for that record to 0.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href=\"http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p></div>"
    },
    {
        "questionNo": 8,
        "questionText": "You are building a prototype for a cryptocurrency news website of a small startup. The website will be deployed to a Spot EC2 Linux instance and will use Amazon Aurora as its database. You requested a spot instance at a maximum price of $0.04/hr which has been fulfilled immediately and after 90 minutes, the spot price increases to $0.06/hr and then your instance was terminated by AWS.\n\nIn this scenario, what would be the total cost of running your spot instance?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\n$0.08"
            },
            {
                "isCorrect": true,
                "text": "​\n\n$0.06\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\n$0.00"
            },
            {
                "isCorrect": false,
                "text": "​\n\n$0.07"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Since the Spot instance has been running for more than an hour, which is past the first instance hour, this means that you will be charged from the time it was launched till the time it was terminated by AWS. The computation for your 90 minute usage would be $0.04 (60 minutes) + $0.02 (30 minutes) = $0.06 hence, the correct answer is $0.06.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/SpotInstance_spotinstancepricinghistory-gwt.png\"></p><p><br></p><p>Based on the official EC2 FAQ:</p><p><strong>Q. How will I be charged if my Spot instance is interrupted?</strong></p><p><em>If your Spot instance is terminated or stopped by Amazon EC2 in the first instance hour, you will not be charged for that usage. However, if you terminate the instance yourself, you will be charged to the nearest second. If the Spot instance is terminated or stopped by Amazon EC2 in any subsequent hour, you will be charged for your usage to the nearest second. If you are running on Windows and you terminate the instance yourself, you will be charged for an entire hour.</em></p><p>Take note that there is one ambiguous <a href=\"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/how-spot-instances-work.html\">AWS document</a> which says:</p><p><em>Spot Instances perform exactly like other EC2 instances while running and can be terminated when you no longer need them. If you terminate your instance, you pay for any partial hour used (as you do for On-Demand or Reserved Instances). However, you are not charged for any partial hour of usage if the Spot price goes above your maximum price and Amazon EC2 interrupts your Spot Instance.</em></p><p>The above paragraph may seem to contradict what the official EC2 FAQ said that you will be charged for your usage to the <strong>nearest second </strong>if the Spot instance is terminated or stopped by Amazon EC2 in any <strong>subsequent hour</strong>. Therefore, please be reminded that the above paragraph is only applicable if the Spot Instance is terminated by Amazon EC2 in the first instance hour only, and not during any subsequent hour (more than 60 minutes) which is what the scenario depicts.</p><p>I have personally contacted AWS about this issue in their documentation:</p><p><a href=\"https://github.com/awsdocs/amazon-ec2-user-guide/pull/65\">https://github.com/awsdocs/amazon-ec2-user-guide/pull/65</a></p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/ec2/faqs/\">https://aws.amazon.com/ec2/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/how-spot-instances-work.html\">https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/how-spot-instances-work.html</a></p></div>"
    },
    {
        "questionNo": 9,
        "questionText": "An e-commerce application is using a fanout messaging pattern for its order management system. For every order, it sends an Amazon SNS message to an SNS topic, and the message is replicated and pushed to multiple Amazon SQS queues for parallel asynchronous processing. A Spot EC2 instance retrieves the message from each SQS queue and processes the message. There was an incident that while an EC2 instance is currently processing a message, the instance was abruptly terminated, and the processing was not completed in time.\n\nIn this scenario, what happens to the SQS message?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\nWhen the message visibility timeout expires, the message becomes available for processing by other EC2 instances\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nThe message is deleted and becomes duplicated in the SQS when the EC2 instance comes online."
            },
            {
                "isCorrect": false,
                "text": "​\n\nThe message will automatically be assigned to the same EC2 instance when it comes back online within or after the visibility timeout."
            },
            {
                "isCorrect": false,
                "text": "​\n\nThe message will be sent to a Dead Letter Queue in AWS DataSync."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A \"fanout\" pattern is when an Amazon SNS message is sent to a topic and then replicated and pushed to multiple Amazon SQS queues, HTTP endpoints, or email addresses. This allows for parallel asynchronous processing. For example, you could develop an application that sends an Amazon SNS message to a topic whenever an order is placed for a product. Then, the Amazon SQS queues that are subscribed to that topic would receive identical notifications for the new order. The Amazon EC2 server instance attached to one of the queues could handle the processing or fulfillment of the order, while the other server instance could be attached to a data warehouse for analysis of all orders received.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/sns/latest/dg/images/sns-fanout.png\"></p><p><br></p><p>When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it.</p><p>Immediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a <em>visibility timeout</em>, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.</p><p>The option that says: <strong><em>The message will automatically be assigned to the same EC2 instance when it comes back online within or after the visibility timeout</em></strong> is incorrect because the message will not be automatically assigned to the same EC2 instance once it is abruptly terminated. When the message visibility timeout expires, the message becomes available for processing by other EC2 instances.</p><p>The option that says: <strong><em>The message is deleted and becomes duplicated in the SQS when the EC2 instance comes online</em></strong> is incorrect because the message will not be deleted and won't be duplicated in the SQS queue when the EC2 instance comes online.</p><p>The option that says: <strong><em>The message will be sent to a Dead Letter Queue in AWS DataSync</em></strong> is incorrect because although the message could be programmatically sent to a Dead Letter Queue (DLQ), it won't be handled by AWS DataSync but by Amazon SQS instead. AWS DataSync is primarily used to simplify your migration with AWS. It makes it simple and fast to move large amounts of data online between on-premises storage and Amazon S3 or Amazon Elastic File System (Amazon EFS).</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-common-scenarios.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-common-scenarios.html</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p></div>"
    },
    {
        "questionNo": 10,
        "questionText": "A leading e-commerce company is in need of a storage solution that can be simultaneously accessed by 1000 Linux servers in multiple availability zones. The servers are hosted in EC2 instances that use a hierarchical directory structure via the NFSv4 protocol. The service should be able to handle the rapidly changing data at scale while still maintaining high performance. It should also be highly durable and highly available whenever the servers will pull data from it, with little need for management.\n\nAs the Solutions Architect, which of the following services is the most cost-effective choice that you should use to meet the above requirement?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nStorage Gateway"
            },
            {
                "isCorrect": true,
                "text": "​\n\nEFS\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nS3"
            },
            {
                "isCorrect": false,
                "text": "​\n\nEBS"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Amazon Web Services (AWS) offers cloud storage services to support a wide range of storage workloads such as EFS, S3 and EBS. You have to understand when you should use Amazon EFS, Amazon S3 and Amazon Elastic Block Store (EBS) based on the specific workloads. In this scenario, the keywords are <strong><em>rapidly changing data</em></strong> and 1000 Linux servers.</p><p>Amazon EFS is a file storage service for use with Amazon EC2. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage for <strong>up to thousands of Amazon EC2 instances</strong>. EFS provides the same level of high availability and high scalability like S3 however, this service is more suitable for scenarios where it is required to have a POSIX-compatible file system or if you are storing rapidly changing data.</p><p><br></p><p><img src=\"https://d1.awsstatic.com/r2018/b/EFS/product-page-diagram-Amazon-EFS-Launch_How-It-Works.cf947858f0ef3557b9fc14077bdf3f65b3f9ff43.png\"></p><p><br></p><p>Data that must be updated very frequently might be better served by storage solutions that take into account read and write latencies, such as Amazon EBS volumes, Amazon RDS, Amazon DynamoDB, Amazon EFS, or relational databases running on Amazon EC2.</p><p>Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest-latency access to data from a single EC2 instance.</p><p>Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere.</p><p>In this scenario, <strong>EFS</strong> is the best answer. As stated above, Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage for <strong>up to thousands of Amazon EC2 instances</strong>. EFS provides the performance, durability, high availability, and storage capacity needed by the 1000 Linux servers in the scenario.</p><p><strong>S3</strong> is incorrect because although this provides the same level of high availability and high scalability like EFS, this service is not suitable for storing data which are rapidly changing, just as mentioned in the above explanation. It is still more effective to use EFS as it offers strong consistency and file locking which the S3 service lacks.</p><p><strong>EBS</strong> is incorrect because an EBS Volume cannot be shared by multiple instances.</p><p><strong>Storage Gateway</strong> is incorrect because this is primarily used to extend the storage of your on-premises data center to your AWS Cloud.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html\">https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html</a></p><p><a href=\"https://aws.amazon.com/efs/features/\">https://aws.amazon.com/efs/features/</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf#page=9\">https://d1.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf#page=9</a></p><p><br></p><p><strong>Check out this Amazon EFS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-efs/?src=udemy\">https://tutorialsdojo.com/amazon-efs/</a></p></div>"
    },
    {
        "questionNo": 11,
        "questionText": "A web application is deployed in an On-Demand EC2 instance in your VPC. There is an issue with the application which requires you to connect to it via an SSH connection. Which of the following is needed in order to access an EC2 instance from the Internet? (Select THREE.)",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nA Private Elastic IP address attached to the EC2 instance."
            },
            {
                "isCorrect": false,
                "text": "​\nA VPN Peering connection."
            },
            {
                "isCorrect": true,
                "text": "​\nAn Internet Gateway (IGW) attached to the VPC.\n(Correct)"
            },
            {
                "isCorrect": true,
                "text": "​\nA Public IP address attached to the EC2 instance.\n(Correct)"
            },
            {
                "isCorrect": true,
                "text": "​\nA route entry to the Internet gateway in the Route table of the VPC.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nA Private IP address attached to the EC2 instance."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In order for you to access your EC2 instance from the Internet, you need to have:</p><p><strong>- An Internet Gateway (IGW) attached to the VPC.</strong></p><p><strong>- A route entry to the Internet gateway in the Route table of the VPC.</strong></p><p><strong>- A Public IP address attached to the EC2 instance.</strong></p><p><br></p><p><img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/scenario-1-ipv6-diagram.png\"></p><p><br></p><p><strong>A Private IP address attached to the EC2 instance</strong> is incorrect as you only use a Private IP inside your VPC.</p><p><strong>A Private Elastic IP address attached to the EC2 instance</strong> is incorrect as an Elastic IP Address is a public IPv4 address, not private. It is reachable from the Internet and is designed for dynamic cloud computing.</p><p><strong>A VPN Peering connection</strong> is incorrect as you only use VPC Peering to connect two VPCs.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html\">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></p></div>"
    },
    {
        "questionNo": 12,
        "questionText": "You have a web application hosted on a fleet of EC2 instances located in two Availability Zones that are all placed behind an Application Load Balancer. As a Solutions Architect, you have to add a health check configuration to ensure your application is highly-available.\n\nWhich health checks will you implement?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nTCP health check"
            },
            {
                "isCorrect": true,
                "text": "​\nHTTP or HTTPS health check\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nFTP health check"
            },
            {
                "isCorrect": false,
                "text": "​\nICMP health check"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A load balancer takes requests from clients and distributes them across the EC2 instances that are registered with the load balancer. You can create a load balancer that listens on both the HTTP (80) and HTTPS (443) ports. If you specify that the HTTPS listener sends requests to the instances on port 80, the load balancer terminates the requests and communication from the load balancer to the instances is not encrypted. If the HTTPS listener sends requests to the instances on port 443, communication from the load balancer to the instances is encrypted.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/images/DefineLB_Protocols_Backend_Auth.png\"></p><p><br></p><p>If your load balancer uses an encrypted connection to communicate with the instances, you can optionally enable authentication of the instances. This ensures that the load balancer communicates with an instance only if its public key matches the key that you specified to the load balancer for this purpose.</p><p>The type of ELB that is mentioned in this scenario is an Application Elastic Load Balancer. This is used if you want a flexible feature set for your web applications with HTTP and HTTPS traffic. Conversely, it only allows 2 types of health check: HTTP and HTTPS.</p><p>Hence, the correct answer is: <strong><em>HTTP or HTTPS health check.</em></strong></p><p><strong><em>ICMP health check</em></strong><em> </em>and <strong><em>FTP health check</em></strong> are incorrect as these are not supported.</p><p><strong><em>TCP health check</em></strong> is incorrect. A TCP health check is only offered in Network Load Balancers and Classic Load Balancers.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html\">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/\">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</a></p><p><br></p><p><strong>EC2 Instance Health Check vs ELB Health Check vs Auto Scaling and Custom Health Check:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-ec2-instance-health-check-vs-elb-health-check-vs-auto-scaling-and-custom-health-check-2/\">https://tutorialsdojo.com/aws-cheat-sheet-ec2-instance-health-check-vs-elb-health-check-vs-auto-scaling-and-custom-health-check-2/</a></p><p><br></p><p><strong>Application Load Balancer vs Network Load Balancer vs Classic Load Balancer:</strong></p><p><a href=\"https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/?src=udemy\">https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/\">https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/</a></p></div>"
    },
    {
        "questionNo": 13,
        "questionText": "You are planning to migrate a MySQL database from your on-premises data center to your AWS Cloud. This database will be used by a legacy batch application which has steady-state workloads in the morning but has its peak load at night for the end-of-day processing. You need to choose an EBS volume which can handle a maximum of 450 GB of data and can also be used as the system boot volume for your EC2 instance. \n\nWhich of the following is the most cost-effective storage type to use in this scenario?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nAmazon EBS Provisioned IOPS SSD (io1)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAmazon EBS Throughput Optimized HDD (st1)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAmazon EBS Cold HDD (sc1)"
            },
            {
                "isCorrect": true,
                "text": "​\n\nAmazon EBS General Purpose SSD (gp2)\n\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, a legacy batch application which has steady-state workloads requires a <strong><em>relational MySQL database</em></strong>. The EBS volume that you should use has to handle a maximum of 450 GB of data and can also be used as the system <strong><em>boot volume</em> </strong>for your EC2 instance. Since HDD volumes cannot be used as a bootable volume, we can narrow down our options by selecting SSD volumes. In addition, SSD volumes are more suitable for transactional database workloads, as shown in the table below:</p><p><img src=\"https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_22-34-15-d1fd30e8eaa8701ddd964e5878e78242.png\"></p><p>General Purpose SSD (<code>gp2</code>) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time. AWS designs <code>gp2</code> volumes to deliver the provisioned performance 99% of the time. A <code>gp2</code> volume can range in size from 1 GiB to 16 TiB.</p><p><strong>Amazon EBS Provisioned IOPS SSD (io1)</strong> is incorrect because this is not the most cost-effective EBS type and is primarily used for critical business applications that require sustained IOPS performance.</p><p><strong>Amazon EBS Throughput Optimized HDD (st1)</strong> is incorrect because this is primarily used for frequently accessed, throughput-intensive workloads. Although it is a low-cost HDD volume, it cannot be used as a system boot volume.</p><p><strong>Amazon EBS Cold HDD (sc1)</strong> is incorrect because although Amazon EBS Cold HDD provides lower cost HDD volume compared to General Purpose SSD, it cannot be used as a system boot volume.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_gp2\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_gp2</a></p><p><br></p><p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</a></p></div>"
    },
    {
        "questionNo": 14,
        "questionText": "The social media company that you are working for needs to capture the detailed information of all HTTP requests that went through their public-facing application load balancer every five minutes. They want to use this data for analyzing traffic patterns and for troubleshooting their web applications in AWS.\n\nWhich of the following options meet the customer requirements?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nEnable AWS CloudTrail for their application load balancer."
            },
            {
                "isCorrect": false,
                "text": "​\nAdd an Amazon CloudWatch Logs agent on the application load balancer."
            },
            {
                "isCorrect": true,
                "text": "​\nEnable access logs on the application load balancer.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nEnable Amazon CloudWatch metrics on the application load balancer."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p> <p>Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. You can disable access logging at any time.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href=\"http://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">http://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p></div>"
    },
    {
        "questionNo": 15,
        "questionText": "A local bank has an in-house application which handles sensitive financial data in a private subnet. After the data is processed by the EC2 worker instances, they will be delivered to S3 for ingestion by other services.\n\nHow should you design this solution so that the data does not pass through the public Internet?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nCreate an Internet gateway in the public subnet with a corresponding route entry that directs the data to S3."
            },
            {
                "isCorrect": true,
                "text": "​\n\nConfigure a VPC Gateway Endpoint along with a corresponding route entry that directs the data to S3.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nConfigure a VPC Interface Endpoint along with a corresponding route entry that directs the data to S3."
            },
            {
                "isCorrect": false,
                "text": "​\n\nProvision a NAT gateway in the private subnet with a corresponding route entry that directs the data to S3."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The important concept that you have to understand in the scenario is that your VPC and your S3 bucket are located within the larger AWS network. However, the traffic coming from your VPC to your S3 bucket is traversing the public Internet by default. To better protect your data in transit, you can set up a VPC endpoint so the incoming traffic from your VPC will not pass through the public Internet, but instead through the private AWS network.</p><p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an Internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other services do not leave the Amazon network.</p><p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components that allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p><p>There are two types of VPC endpoints: <em>interface endpoints</em> and <em>gateway endpoints</em>. You should create the type of VPC endpoint required by the supported service. As a rule of thumb, most AWS services use VPC <em>Interface</em> Endpoint except for S3 and DynamoDB, which use VPC <em>Gateway</em> Endpoint.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/vpc-endpoint-s3-diagram.png\"></p><p><br></p><p><strong>Configuring a VPC Gateway Endpoint along with a corresponding route entry that directs the data to S3</strong> is correct because VPC Gateway Endpoint supports private connection to S3.</p><p><strong>Creating an Internet gateway in the public subnet with a corresponding route entry that directs the data to S3</strong> is incorrect because Internet gateway is used for instances in the public subnet to have accessibility to the Internet.</p><p><strong>Configuring a VPC Interface Endpoint along with a corresponding route entry that directs the data to S3</strong> is incorrect because VPC Interface Endpoint does not support the S3 service. You should use a VPC Gateway Endpoint instead. As mentioned in the above explanation, most AWS services use VPC <em>Interface</em> Endpoint except for S3 and DynamoDB, which use VPC <em>Gateway</em> Endpoint.</p><p><strong>Provisioning a NAT gateway in the private subnet with a corresponding route entry that directs the data to S3</strong> is incorrect because NAT Gateway allows instances in the private subnet to gain access to the Internet, but not vice versa.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></p></div>"
    },
    {
        "questionNo": 16,
        "questionText": "A Junior DevOps Engineer deployed a large EBS-backed EC2 instance to host a NodeJS web app in AWS which was developed by an IT contractor. He properly configured the security group and used a key pair named \"tutorialsdojokey\" which has a tutorialsdojokey.pem private key file. The EC2 instance works as expected and the junior DevOps engineer can connect to it using an SSH connection. The IT contractor was also given the key pair and he has made various changes in the instance as well to the files located in .ssh folder to make the NodeJS app work. After a few weeks, the IT contractor and the junior DevOps engineer cannot connect the EC2 instance anymore, even with a valid private key file. They are constantly getting a \"Server refused our key\" error even though their private key is valid.\n\nIn this scenario, which one of the following options is not a possible reason for this issue?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nThe SSH private key that you are using has a file permission of 0777.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nYou're using an SSH private key but the corresponding public key is not in the authorized_keys file."
            },
            {
                "isCorrect": false,
                "text": "​\n\nYou don't have permissions for the .ssh folder."
            },
            {
                "isCorrect": false,
                "text": "​\n\nYou don't have permissions for your authorized_keys file."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>All of the options here are correct except for the option that says: <strong>The SSH private key that you are using has a file permission of 0777</strong> because if the private key that you are using has a file permission of 0777, then it will throw an \"Unprotected Private Key File\" error and not a \"Server refused our key\" error.</p><p>You might be unable to log into an EC2 instance if:</p><p>- You're using an SSH private key but the corresponding public key is not in the authorized_keys file.</p><p>- You don't have permissions for your authorized_keys file.</p><p>- You don't have permissions for the .ssh folder.</p><p>- Your authorized_keys file or .ssh folder isn't named correctly.</p><p>- Your authorized_keys file or .ssh folder was deleted.</p><p>- Your instance was launched without a key, or it was launched with an incorrect key.</p><p>To connect to your EC2 instance after receiving the error \"Server refused our key,\" you can update the instance's user data to append the specified SSH public key to the authorized_keys file, which sets the appropriate ownership and file permissions for the SSH directory and files contained in it.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-server-refused-our-key/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-server-refused-our-key/</a></p></div>"
    },
    {
        "questionNo": 17,
        "questionText": "You are setting up a cost-effective architecture for a log processing application which has frequently accessed, throughput-intensive workloads with large, sequential I/O operations. The application should be hosted in an already existing On-Demand EC2 instance in your VPC. You have to attach a new EBS volume that will be used by the application.\n\nWhich of the following is the most suitable EBS volume type that you should use in this scenario?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nEBS Cold HDD (sc1)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nEBS General Purpose SSD (gp2)"
            },
            {
                "isCorrect": true,
                "text": "​\n\nEBS Throughput Optimized HDD (st1)\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nEBS Provisioned IOPS SSD (io1)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In the exam, always consider the difference between SSD and HDD as shown on the table below. This will allow you to easily eliminate specific EBS-types in the options which are not SSD or not HDD, depending on whether the question asks for a storage type which has <strong><em>small, random</em></strong> I/O operations or <strong><em>large, sequential</em></strong> I/O operations.</p><p>Since the scenario has workloads with large, sequential I/O operations, we can narrow down our options by selecting HDD volumes, instead of SDD volumes which are more suitable for small, random I/O operations.</p><p><br></p><p><img src=\"https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_22-34-15-d1fd30e8eaa8701ddd964e5878e78242.png\"></p><p><br></p><p><strong>Throughput Optimized HDD (</strong><code><strong>st1</strong></code><strong>)</strong> volumes provide low-cost magnetic storage that defines performance in terms of throughput rather than IOPS. This volume type is a good fit for large, sequential workloads such as Amazon EMR, ETL, data warehouses, and log processing. Bootable <code>st1</code> volumes are not supported.</p><p>Throughput Optimized HDD (<code>st1</code>) volumes, though similar to Cold HDD (<code>sc1</code>) volumes, are designed to support <em>frequently</em> accessed data.</p><p><strong>EBS Provisioned IOPS SSD (io1)</strong> is incorrect because Amazon EBS Provisioned IOPS SSD is not the most cost-effective EBS type and is primarily used for critical business applications that require sustained IOPS performance.</p><p><strong>EBS General Purpose SSD (gp2)</strong> is incorrect because although an Amazon EBS General Purpose SSD volume balances price and performance for a wide variety of workloads, it is not suitable for frequently accessed, throughput-intensive workloads. Throughput Optimized HDD is a more suitable option to use than General Purpose SSD.</p><p><strong>EBS Cold HDD (sc1)</strong> is incorrect because although this provides lower cost HDD volume compared to General Purpose SSD, it is much suitable for <strong><em>less</em></strong> frequently accessed workloads.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_st1\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_st1</a></p><p><br></p><p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</a></p></div>"
    },
    {
        "questionNo": 18,
        "questionText": "You created a new CloudFormation template that creates 4 EC2 instances and are connected to one Elastic Load Balancer (ELB). Which section of the template should you configure to get the Domain Name Server hostname of the ELB upon the creation of the AWS stack?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nParameters"
            },
            {
                "isCorrect": false,
                "text": "​\nMappings"
            },
            {
                "isCorrect": false,
                "text": "​\nResources"
            },
            {
                "isCorrect": true,
                "text": "​\nOutputs\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"> <p><strong>Outputs</strong>&nbsp;is an optional section of the CloudFormation template that describes the values that are returned whenever you view your stack's properties.&nbsp;</p> <p>&nbsp;</p>  <p><strong>Reference:</strong></p> <p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p> <p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudFormation Cheat Sheet:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/</span></a></p></div>"
    },
    {
        "questionNo": 19,
        "questionText": "You are a working as a Solutions Architect for a fast-growing startup which just started operations during the past 3 months. They currently have an on-premises Active Directory and 10 computers. To save costs in procuring physical workstations, they decided to deploy virtual desktops for their new employees in a virtual private cloud in AWS. The new cloud infrastructure should leverage on the existing security controls in AWS but can still communicate with their on-premises network. \n\nWhich set of AWS services will you use to meet these requirements?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nAWS Directory Services, VPN connection, and AWS Identity and Access Management"
            },
            {
                "isCorrect": true,
                "text": "​\nAWS Directory Services, VPN connection, and Amazon Workspaces\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nAWS Directory Services, VPN connection, and ClassicLink"
            },
            {
                "isCorrect": false,
                "text": "​\nAWS Directory Services, VPN connection, and Amazon S3"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>For this scenario, the best answer is: <strong>AWS Directory Services, VPN connection, and Amazon Workspaces</strong>.</p><p>First, you need a VPN connection to connect the VPC and your on-premises network. Second, you need AWS Directory Services to integrate with your on-premises Active Directory and lastly, you need to use Amazon Workspace to create the needed virtual desktops in your VPC.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/directoryservice/\">https://aws.amazon.com/directoryservice/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html</a></p><p><a href=\"https://aws.amazon.com/workspaces/\">https://aws.amazon.com/workspaces/</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Directory Service, Amazon VPC and Amazon WorkSpaces:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-aws-directory-service/\">https://tutorialsdojo.com/aws-cheat-sheet-aws-directory-service/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-workspaces/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-workspaces/</a></p></div>"
    },
    {
        "questionNo": 20,
        "questionText": "You are assigned to design a highly available architecture in AWS. You have two target groups with three EC2 instances each, which are added to an Application Load Balancer. In the security group of the EC2 instance, you have verified that the port 80 for HTTP is allowed. However, the instances are still showing out of service from the load balancer.\n\nWhat could be the root cause of this issue?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\nThe health check configuration is not properly defined.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nThe wrong instance type was used for the EC2 instance."
            },
            {
                "isCorrect": false,
                "text": "​\nThe wrong subnet was used in your VPC"
            },
            {
                "isCorrect": false,
                "text": "​\nThe instances are using the wrong AMI."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Since the security group is properly configured, the issue may be caused by a wrong&nbsp;health check configuration in the Target Group.&nbsp;</p> <p>Your Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called&nbsp;<em>health checks</em>. Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target group with which the target is registered. After your target is registered, it must pass one health check to be considered healthy. After each health check is completed, the load balancer node closes the connection that was established for the health check.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href=\"http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html\">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p> <p>&nbsp;</p> <p><strong>Application Load Balancer vs Network Load Balancer vs Classic Load Balancer:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/</span></a></p></div>"
    },
    {
        "questionNo": 21,
        "questionText": "You have an On-Demand EC2 instance with an attached non-root EBS volume. There is a scheduled job that creates a snapshot of this EBS volume every midnight at 12 AM when the instance is not used. On one night, there's been a production incident where you need to perform a change on both the instance and on the EBS volume at the same time, when the snapshot is currently taking place.\n\nWhich of the following scenario is true when it comes to the usage of an EBS volume while the snapshot is in progress?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nThe EBS volume cannot be detached or attached to an EC2 instance until the snapshot completes"
            },
            {
                "isCorrect": false,
                "text": "​\nThe EBS volume cannot be used until the snapshot completes."
            },
            {
                "isCorrect": false,
                "text": "​\nThe EBS volume can be used in read-only mode while the snapshot is in progress."
            },
            {
                "isCorrect": true,
                "text": "​\nThe EBS volume can be used while the snapshot is in progress.\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Snapshots occur asynchronously; the point-in-time snapshot is created immediately, but the status of the snapshot is <code>pending</code> until the snapshot is complete (when all of the modified blocks have been transferred to Amazon S3), which can take several hours for large initial snapshots or subsequent snapshots where many blocks have changed.</p><p>While it is completing, an in-progress snapshot is not affected by ongoing reads and writes to the volume hence, you can still use the EBS volume normally.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/images/snapshot_1a.png\"></p><p>When you create an EBS volume based on a snapshot, the new volume begins as an exact replica of the original volume that was used to create the snapshot. The replicated volume loads data lazily in the background so that you can begin using it immediately. If you access data that hasn't been loaded yet, the volume immediately downloads the requested data from Amazon S3, and then continues loading the rest of the volume's data in the background.</p><p>A non-root EBS volume can be detached or attached to a new EC2 instance while the snapshot is in progress. The only exception here is if you are taking a snapshot of your root volume.</p><p>Hence, the correct answer is: <strong>The EBS volume can be used while the snapshot is in progress.</strong></p><p>The option that says:<strong> The EBS volume cannot be detached or attached to an EC2 instance until the snapshot completes</strong> is not entirely correct. A non-root EBS volume can be detached or attached to a new EC2 instance while the snapshot is in progress. However, you cannot do this for your root volume.</p><p>The option that says: <strong>The EBS volume can be used in read-only mode while the snapshot is in progress</strong> is incorrect because you can perform both read and write operations in the volume while the snapshot is in progress.</p><p>The option that says: <strong>The EBS volume cannot be used until the snapshot completes</strong> is incorrect because just as shown in the previous option, the volume can be used even if the snapshot process is in progress.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html</a></p><p><br></p><p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</a></p></div>"
    },
    {
        "questionNo": 22,
        "questionText": "A company is planning to launch a High Performance Computing (HPC) cluster in AWS that does Computational Fluid Dynamics (CFD) simulations. The solution should scale-out their simulation jobs to experiment with more tunable parameters for faster and more accurate results. The cluster is composed of Windows servers hosted on t3a.medium EC2 instances. As the Solutions Architect, you should ensure that the architecture provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies.\n\nWhich is the MOST suitable and cost-effective solution that the Architect should implement to achieve the above requirements?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nEnable Enhanced Networking with Intel 82599 Virtual Function (VF) interface on the Windows EC2 Instances."
            },
            {
                "isCorrect": false,
                "text": "​\n\nEnable Enhanced Networking with Elastic Fabric Adapter (EFA) on the Windows EC2 Instances."
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse AWS ParallelCluster to deploy and manage the HPC cluster to provide higher bandwidth, higher packet per second (PPS) performance, and lower inter-instance latencies."
            },
            {
                "isCorrect": true,
                "text": "​\n\nEnable Enhanced Networking with Elastic Network Adapter (ENA) on the Windows EC2 Instances.\n\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Enhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types. SR-IOV is a method of device virtualization that provides higher I/O performance and lower CPU utilization when compared to traditional virtualized network interfaces. Enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies. There is no additional charge for using enhanced networking.</p><p><br></p><p><img src=\"https://udemy-images.s3.amazonaws.com/redactor/raw/2020-01-10_11-44-29-f938be2240ccd0493d45bd80ca276619.png\"></p><p><br></p><p>Amazon EC2 provides enhanced networking capabilities through the Elastic Network Adapter (ENA). It supports network speeds of up to 100 Gbps for supported instance types. Elastic Network Adapters (ENAs) provide traditional IP networking features that are required to support VPC networking.</p><p>An Elastic Fabric Adapter (EFA) is simply an Elastic Network Adapter (ENA) with added capabilities. It provides all of the functionality of an ENA, with additional OS-bypass functionality. OS-bypass is an access model that allows HPC and machine learning applications to communicate directly with the network interface hardware to provide low-latency, reliable transport functionality.</p><p>The OS-bypass capabilities of EFAs are not supported on Windows instances. If you attach an EFA to a Windows instance, the instance functions as an Elastic Network Adapter, without the added EFA capabilities.</p><p>Hence, the correct answer is to <strong>enable Enhanced Networking with Elastic Network Adapter (ENA) on the Windows EC2 Instances</strong>.</p><p><strong>Enabling Enhanced Networking with Elastic Fabric Adapter (EFA) on the Windows EC2 Instances</strong> is incorrect because the OS-bypass capabilities of the Elastic Fabric Adapter (EFA) are not supported on Windows instances. Although you can attach EFA to your Windows instances, this will just act as a regular Elastic Network Adapter, without the added EFA capabilities. Moreover, it doesn't support the t3a.medium instance type that is being used in the HPC cluster.</p><p><strong>Enabling Enhanced Networking with Intel 82599 Virtual Function (VF) interface on the Windows EC2 Instances</strong> is incorrect because although you can attach an Intel 82599 Virtual Function (VF) interface on your Windows EC2 Instances to improve its networking capabilities, it doesn't support the t3a.medium instance type that is being used in the HPC cluster.</p><p><strong>Using AWS ParallelCluster to deploy and manage the HPC cluster to provide higher bandwidth, higher packet per second (PPS) performance, and lower inter-instance latencies</strong> is incorrect because an AWS ParallelCluster is just an AWS-supported open-source cluster management tool that makes it easy for you to deploy and manage High Performance Computing (HPC) clusters on AWS. It does not provide higher bandwidth, higher packet per second (PPS) performance, and lower inter-instance latencies, unlike ENA or EFA.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html</a></p></div>"
    },
    {
        "questionNo": 23,
        "questionText": "You have EC2 instances running on your VPC. You have both UAT and production EC2 instances running. You want to ensure that employees who are responsible for the UAT instances don’t have the access to work on the production instances to minimize security risks.\n\nWhich of the following would be the best way to achieve this?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nLaunch the UAT and production instances in different Availability Zones and use Multi Factor Authentication."
            },
            {
                "isCorrect": false,
                "text": "​\n\nProvide permissions to the users via the AWS Resource Access Manager (RAM) service to only access EC2 instances that are used for production or development."
            },
            {
                "isCorrect": false,
                "text": "​\nLaunch the UAT and production EC2 instances in separate VPC's connected by VPC peering."
            },
            {
                "isCorrect": true,
                "text": "​\nDefine the tags on the UAT and production servers and add a condition to the IAM policy which allows access to specific tags.\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>For this scenario, the best way to achieve the required solution is to use a combination of Tags and IAM policies. You can define the tags on the UAT and production EC2 instances and add a condition to the IAM policy which allows access to specific tags.</p><p>Tags enable you to categorize your AWS resources in different ways, for example, by purpose, owner, or environment. This is useful when you have many resources of the same type — you can quickly identify a specific resource based on the tags you've assigned to it.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/Tag_Example.png\"></p><p><br></p><p>By default, IAM users don't have permission to create or modify Amazon EC2 resources, or perform tasks using the Amazon EC2 API. (This means that they also can't do so using the Amazon EC2 console or CLI.) To allow IAM users to create or modify resources and perform tasks, you must create IAM policies that grant IAM users permission to use the specific resources and API actions they'll need, and then attach those policies to the IAM users or groups that require those permissions.</p><p>Hence, the correct answer is:<strong><em> Define the tags on the UAT and production servers and add a condition to the IAM policy which allows access to specific tags.</em></strong></p><p>The option that says:<strong><em> Launch the UAT and production EC2 instances in separate VPC's connected by VPC peering</em></strong> is incorrect because these are just network changes to your cloud architecture and doesn't have any effect on the security permissions of your users to access your EC2 instances.</p><p>The option that says: <strong><em>Provide permissions to the users via the AWS Resource Access Manager (RAM) service to only access EC2 instances that are used for production or development</em></strong> is incorrect because the AWS Resource Access Manager (RAM) is primarily used to securely share your resources across AWS accounts or within your Organization and not on a single AWS account. You also have to set up a custom IAM Policy in order for this to work.</p><p>The option that says: <strong><em>Launch the UAT and production instances in different Availability Zones and use Multi Factor Authentication</em></strong> is incorrect because placing the EC2 instances to different AZs will only improve the availability of the systems but won't have any significance in terms of security. You have to set up an IAM Policy that allows access to EC2 instances based on their tags. In addition, a Multi-Factor Authentication is not a suitable security feature to be implemented for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-policies-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-policies-for-amazon-ec2.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
    },
    {
        "questionNo": 24,
        "questionText": "A health organization is using a large Dedicated EC2 instance with multiple EBS volumes to host its health records web application. The EBS volumes must be encrypted due to the confidentiality of the data that they are handling and also to comply with the HIPAA (Health Insurance Portability and Accountability Act) standard.   \n\nIn EBS encryption, what service does AWS use to secure the volume's data at rest? (Select TWO.)",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nBy using a password stored in CloudHSM."
            },
            {
                "isCorrect": true,
                "text": "​\n\nBy using your own keys in AWS Key Management Service (KMS).\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nBy using S3 Client-Side Encryption."
            },
            {
                "isCorrect": false,
                "text": "​\n\nBy using the SSL certificates provided by the AWS Certificate Manager (ACM)."
            },
            {
                "isCorrect": true,
                "text": "​\n\nBy using Amazon-managed keys in AWS Key Management Service (KMS).\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nBy using S3 Server-Side Encryption."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Amazon EBS encryption offers seamless encryption of EBS data volumes, boot volumes, and snapshots, eliminating the need to build and maintain a secure key management infrastructure. EBS encryption enables data at rest security by encrypting your data using Amazon-managed keys, or keys you create and manage using the AWS Key Management Service (KMS). The encryption occurs on the servers that host EC2 instances, providing encryption of data as it moves between EC2 instances and EBS storage.</p><p>Hence, the correct answers are: <strong>using your own keys in AWS Key Management Service (KMS)</strong> and <strong>using Amazon-managed keys in AWS Key Management Service (KMS).</strong></p><p><strong>Using S3 Server-Side Encryption</strong> and <strong>using S3 Client-Side Encryption</strong> are both incorrect as these relate only to S3.</p><p><strong>Using a password stored in CloudHSM</strong> is incorrect as you only store keys in CloudHSM and not passwords.</p><p><strong>Using the SSL certificates provided by the AWS Certificate Manager (ACM)</strong> is incorrect as ACM only provides SSL certificates and not data encryption of EBS Volumes.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/ebs/faqs/\">https://aws.amazon.com/ebs/faqs/</a></p><p><br></p><p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</a></p></div>"
    },
    {
        "questionNo": 25,
        "questionText": "Your IT Director instructed you to ensure that all of the AWS resources in your VPC don’t go beyond their respective service limits. You should prepare a system that provides you real-time guidance in provisioning your resources that adheres to the AWS best practices.\n\nWhich of the following is the MOST appropriate service to use to satisfy this task?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nAWS Budgets"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAmazon Inspector"
            },
            {
                "isCorrect": true,
                "text": "​\nAWS Trusted Advisor\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAWS Cost Explorer"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Trusted Advisor</strong> is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. It inspects your AWS environment and makes recommendations for saving money, improving system performance and reliability, or closing security gaps.</p><p>Whether establishing new workflows, developing applications, or as part of ongoing improvement, take advantage of the recommendations provided by Trusted Advisor on a regular basis to help keep your solutions provisioned optimally.</p><p><br></p><p><img src=\"https://d1.awsstatic.com/product-marketing/AWS%20Support/AWS-trusted-advisor.5b9909d5f29f680eeb12ccff536e8d88d8701304.png\"></p><p><br></p><p>Trusted Advisor includes an ever-expanding list of checks in the following five categories:</p><p><strong>Cost Optimization </strong>– recommendations that can potentially save you money by highlighting unused resources and opportunities to reduce your bill.</p><p><strong>Security</strong> – identification of security settings that could make your AWS solution less secure.</p><p><strong>Fault Tolerance</strong> – recommendations that help increase the resiliency of your AWS solution by highlighting redundancy shortfalls, current service limits, and over-utilized resources.</p><p><strong>Performance</strong> – recommendations that can help to improve the speed and responsiveness of your applications.</p><p><strong>Service Limits</strong> – recommendations that will tell you when service usage is more than 80% of the service limit.</p><p><br></p><p>Hence, the correct answer in this scenario is <strong>AWS Trusted Advisor</strong>.</p><p><strong>AWS Cost Explorer</strong> is incorrect because this is just a tool that enables you to view and analyze your costs and usage. You can explore your usage and costs using the main graph, the Cost Explorer cost and usage reports, or the Cost Explorer RI reports. It has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time.</p><p><strong>AWS Budgets</strong> is incorrect because it simply gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. You can also use AWS Budgets to set reservation utilization or coverage targets and receive alerts when your utilization drops below the threshold you define.</p><p><strong>Amazon Inspector</strong> is incorrect because it is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/faqs/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/faqs/</a></p><p><br></p><p><strong>Check out this AWS Trusted Advisor Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-trusted-advisor/?src=udemy\">https://tutorialsdojo.com/aws-trusted-advisor/</a></p></div>"
    },
    {
        "questionNo": 26,
        "questionText": "You recently launched a fleet of on-demand EC2 instances to host a massively multiplayer online role-playing game (MMORPG) server in your VPC. The EC2 instances are configured with Auto Scaling and AWS Systems Manager. What can you use to configure your EC2 instances without having to establish a RDP or SSH connection to each instance?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nEC2Config"
            },
            {
                "isCorrect": false,
                "text": "​\nAWS Config"
            },
            {
                "isCorrect": true,
                "text": "​\n\nRun Command\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nAWS CodePipeline"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use Run Command from the console to configure instances without having to login to each instance.</p><p>AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. A <em>managed instance</em> is any Amazon EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
    },
    {
        "questionNo": 27,
        "questionText": "A news company is planning to use a Hardware Security Module (CloudHSM) in AWS for secure key storage of their web applications. You have launched the CloudHSM cluster but after just a few hours, a support staff mistakenly attempted to log in as the administrator three times using an invalid password in the Hardware Security Module. This has caused the HSM to be zeroized, which means that the encryption keys on it have been wiped. Unfortunately, you did not have a copy of the keys stored anywhere else.\n\nHow can you obtain a new copy of the keys that you have stored on Hardware Security Module?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nUse the Amazon CLI to get a copy of the keys."
            },
            {
                "isCorrect": true,
                "text": "​\nThe keys are lost permanently if you did not have a copy.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nContact AWS Support and they will provide you a copy of the keys."
            },
            {
                "isCorrect": false,
                "text": "​\nRestore a snapshot of the Hardware Security Module."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Attempting to log in as the administrator more than twice with the wrong password zeroizes your HSM appliance.&nbsp;When an HSM is zeroized, all keys, certificates, and other data on the HSM is destroyed. You can use your cluster's security group to prevent an unauthenticated user from zeroizing your HSM.</p> <p>&nbsp;</p> <p><img src=\"https://d1.awsstatic.com/product-marketing/cloudhsm/CloudHSM_Diagrams_2-final.6f427ebb14d021b9cd6120aeee72cf4d3723e89b.png\"></p> <p>&nbsp;</p> <p>Amazon does not have access to your keys nor to the credentials of your&nbsp;Hardware Security Module (HSM) and therefore has no way to recover your keys if you lose your credentials.&nbsp;Amazon strongly recommends that you use two or more HSMs in separate Availability Zones in any production CloudHSM Cluster to avoid loss of cryptographic keys.</p> <p>Refer to the CloudHSM FAQs for reference:&nbsp;</p> <p><strong>Q: Could I lose my keys if a single HSM instance fails?</strong></p> <p>Yes. It is possible to lose keys that were created since the most recent daily backup if the CloudHSM cluster that you are using fails and you are not using two or more HSMs. Amazon strongly recommends that you use two or more HSMs, in separate Availability Zones, in any production CloudHSM Cluster to avoid loss of cryptographic keys.</p> <p><strong>Q: Can Amazon recover my keys if I lose my credentials to my HSM?</strong></p> <p>No. Amazon does not have access to your keys or credentials and therefore has no way to recover your keys if you lose your credentials.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/stop-cloudhsm/\">https://aws.amazon.com/premiumsupport/knowledge-center/stop-cloudhsm/</a></p> <p><a href=\"https://aws.amazon.com/cloudhsm/faqs/\">https://aws.amazon.com/cloudhsm/faqs/</a></p> <p><a href=\"https://d1.awsstatic.com/whitepapers/Security/security-of-aws-cloudhsm-backups.pdf\">https://d1.awsstatic.com/whitepapers/Security/security-of-aws-cloudhsm-backups.pdf</a></p></div>"
    },
    {
        "questionNo": 28,
        "questionText": "In a startup company you are working for, you are asked to design a web application that requires a NoSQL database that has no limit on the storage size for a given table. The startup is still new in the market and it has very limited human resources who can take care of the database infrastructure.\n\nWhich is the most suitable service that you can implement that provides a fully managed, scalable and highly available NoSQL service?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nAmazon Aurora"
            },
            {
                "isCorrect": true,
                "text": "​\nDynamoDB\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAmazon Neptune"
            },
            {
                "isCorrect": false,
                "text": "​\nSimpleDB"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The term \"fully managed\" means that Amazon will manage the underlying infrastructure of the service hence, you don't need an additional human resource to support or maintain the service. Therefore, Amazon DynamoDB is the right answer. Remember that Amazon RDS is a managed service but not \"fully managed\" as you still have the option to maintain and configure the underlying server of the database.</p><p><strong>Amazon DynamoDB</strong> is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed cloud database and supports both document and key-value store models. Its flexible data model, reliable performance, and automatic scaling of throughput capacity make it a great fit for mobile, web, gaming, ad tech, IoT, and many other applications.</p><p><strong>Amazon Neptune</strong> is incorrect because this is primarily used as a graph database.</p><p><strong>Amazon Aurora</strong> is incorrect because this is a relational database and not a NoSQL database.</p><p><strong>SimpleDB</strong> is incorrect because although SimpleDB is also a highly available and scalable NoSQL database, it has a limit on the request capacity or storage size for a given table, unlike DynamoDB.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</a></p></div>"
    },
    {
        "questionNo": 29,
        "questionText": "You are working for a large financial company. In their enterprise application, they want to apply a group of database-specific settings to their Relational Database Instances.   \n\nWhich of the following options can be used to easily apply the settings in one go for all of the Relational database instances?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\nParameter Groups\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nSecurity Groups"
            },
            {
                "isCorrect": false,
                "text": "​\nNACL Groups"
            },
            {
                "isCorrect": false,
                "text": "​\nIAM Roles"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You manage your DB engine configuration through the use of parameters in a DB parameter group. DB parameter groups act as a&nbsp;<em>container</em>&nbsp;for engine configuration values that are applied to one or more DB instances.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p></div>"
    },
    {
        "questionNo": 30,
        "questionText": "Your company has a web-based ticketing service that utilizes Amazon SQS and a fleet of EC2 instances. The EC2 instances that consume messages from the SQS queue are configured to poll the queue as often as possible to keep end-to-end throughput as high as possible. You noticed that polling the queue in tight loops is using unnecessary CPU cycles, resulting in increased operational costs due to empty responses.   \n\nIn this scenario, what will you do to make the system more cost-effective?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nConfigure Amazon SQS to use short polling by setting the ReceiveMessageWaitTimeSeconds to a number greater than zero."
            },
            {
                "isCorrect": false,
                "text": "​\nConfigure Amazon SQS to use short polling by setting the ReceiveMessageWaitTimeSeconds to zero."
            },
            {
                "isCorrect": true,
                "text": "​\nConfigure Amazon SQS to use long polling by setting the ReceiveMessageWaitTimeSeconds to a number greater than zero.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nConfigure Amazon SQS to use long polling by setting the ReceiveMessageWaitTimeSeconds to zero."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the application is deployed in a fleet of EC2 instances that are polling messages from a single SQS queue. Amazon SQS uses short polling by default, querying only a subset of the servers (based on a weighted random distribution) to determine whether any messages are available for inclusion in the response. Short polling works for scenarios that require higher throughput. However, you can also configure the queue to use Long polling instead, to reduce cost.</p><p>The ReceiveMessageWaitTimeSeconds is the queue attribute that determines whether you are using Short or Long polling. By default, its value is zero which means it is using Short polling. If it is set to a value greater than zero, then it is Long polling.</p><p>Hence, <strong>configuring Amazon SQS to use long polling by setting the ReceiveMessageWaitTimeSeconds to a number greater than zero is the correct answer</strong>.</p><p>Quick facts about SQS Long Polling:</p><p>- Long polling helps reduce your cost of using Amazon SQS by reducing the number of empty responses when there are no messages available to return in reply to a <em>ReceiveMessage</em> request sent to an Amazon SQS queue and eliminating false empty responses when messages are available in the queue but aren't included in the response.</p><p>- Long polling reduces the number of empty responses by allowing Amazon SQS to wait until a message is available in the queue before sending a response. Unless the connection times out, the response to the <code>ReceiveMessage</code> request contains at least one of the available messages, up to the maximum number of messages specified in the <code>ReceiveMessage</code> action.</p><p>- Long polling eliminates false empty responses by querying all (rather than a limited number) of the servers. Long polling returns messages as soon any message becomes available.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</a></p></div>"
    },
    {
        "questionNo": 31,
        "questionText": "An investment bank has a distributed batch processing application which is hosted in an Auto Scaling group of Spot EC2 instances with an SQS queue. You configured your components to use client-side buffering so that the calls made from the client will be buffered first and then sent as a batch request to SQS. What is a period of time during which the SQS queue prevents other consuming components from receiving and processing a message?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nReceiving Timeout"
            },
            {
                "isCorrect": false,
                "text": "​\nProcessing Timeout"
            },
            {
                "isCorrect": false,
                "text": "​\nComponent Timeout"
            },
            {
                "isCorrect": true,
                "text": "​\nVisibility Timeout\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a message.&nbsp;</p> <p>When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it.</p> <p>Immediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a&nbsp;<strong><em>visibility timeout</em></strong>, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p> <p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p></div>"
    },
    {
        "questionNo": 32,
        "questionText": "You are working for a top IT Consultancy that has a VPC with two On-Demand EC2 instances with Elastic IP addresses. You were notified that your EC2 instances are currently under SSH brute force attacks over the Internet. Their IT Security team has identified the IP addresses where these attacks originated. You have to immediately implement a temporary fix to stop these attacks while the team is setting up AWS WAF, GuardDuty, and AWS Shield Advanced to permanently fix the security vulnerability.\n\nWhich of the following provides the quickest way to stop the attacks to your instances?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\nBlock the IP addresses in the Network Access Control List\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nRemove the Internet Gateway from the VPC"
            },
            {
                "isCorrect": false,
                "text": "​\nPlace the EC2 instances into private subnets"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAssign a static Anycast IP address to each EC2 instance"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A <strong>network access control list (ACL)</strong> is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/security-diagram.png\"></p><p><br></p><p>The following are the basic things that you need to know about network ACLs:</p><p>- Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.</p><p>- You can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</p><p>- Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</p><p>- You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</p><p>- A network ACL contains a numbered list of rules that we evaluate in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL. The highest number that you can use for a rule is 32766. We recommend that you start by creating rules in increments (for example, increments of 10 or 100) so that you can insert new rules where you need to later on.</p><p>- A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic.</p><p>- Network ACLs are stateless; responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).</p><p><br></p><p>The scenario clearly states that it requires the <strong>quickest</strong> way to fix the security vulnerability. In this situation, you can manually block the offending IP addresses using Network ACLs since the IT Security team already identified the list of offending IP addresses. Alternatively, you can set up a bastion host however, this option entails additional time to properly set up as you have to configure the security configurations of your bastion host.</p><p>Hence, <strong>blocking the IP addresses in the Network Access Control List</strong> is the best answer since it can quickly resolve the issue by blocking the IP addresses using Network ACL.</p><p><strong>Placing the EC2 instances into private subnets</strong> is incorrect because if you deploy the EC2 instance in the private subnet without public or EIP address, it would not be accessible over the Internet, even to you.</p><p><strong>Removing the Internet Gateway from the VPC</strong> is incorrect because doing this will also make your EC2 instance inaccessible to you as it will cut down the connection to the Internet.</p><p><strong>Assigning a static Anycast IP address to each EC2 instance</strong> is incorrect because a static Anycast IP address is primarily used by AWS Global Accelerator to enable organizations to seamlessly route traffic to multiple regions and improve availability and performance for their end-users.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html</a></p><p><br></p><p><strong>Security Group vs NACL:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p><p><a href=\"https://tutorialsdojo.com/security-group-vs-nacl/?src=udemy\">https://tutorialsdojo.com/security-group-vs-nacl/</a></p></div>"
    },
    {
        "questionNo": 33,
        "questionText": "An application is hosted in an On-Demand EC2 instance and is using Amazon SDK to communicate to other AWS services such as S3, DynamoDB, and many others. As part of the upcoming IT audit, you need to ensure that all API calls to your AWS resources are logged and durably stored. \n\nWhich is the most suitable service that you should use to meet this requirement?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nAmazon CloudWatch"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAmazon API Gateway"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAWS X-Ray"
            },
            {
                "isCorrect": true,
                "text": "​\n\nAWS CloudTrail\n\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>AWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred.</p><p><strong>Amazon CloudWatch</strong> is incorrect because this is primarily used for systems monitoring based on the server metrics. It does not have the capability to track API calls to your AWS resources.</p><p><strong>AWS X-Ray</strong> is incorrect because this is usually used to debug and analyze your microservices applications with request tracing so you can find the root cause of issues and performance. Unlike CloudTrail, it does not record the API calls that were made to your AWS resources.</p><p><strong>Amazon API Gateway</strong> is incorrect because this is not used for logging each and every API call to your AWS resources. It is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/\">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/</a></p></div>"
    },
    {
        "questionNo": 34,
        "questionText": "An online shopping platform is hosted on an Auto Scaling group of On-Demand EC2 instances with a default Auto Scaling termination policy and no instance protection configured. The system is deployed across three Availability Zones in the US West region (us-west-1) with an Application Load Balancer in front to provide high availability and fault tolerance for the shopping platform. The us-west-1a, us-west-1b, and us-west-1c Availability Zones have 10, 8 and 7 running instances respectively. Due to the low number of incoming traffic, the scale-in operation has been triggered.   \n\nWhich of the following will the Auto Scaling group do to determine which instance to terminate first in this scenario? (Select THREE.)",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nSelect the instance that is closest to the next billing hour.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nChoose the Availability Zone with the least number of instances, which is the us-west-1c Availability Zone in this scenario."
            },
            {
                "isCorrect": true,
                "text": "​\n\nChoose the Availability Zone with the most number of instances, which is the us-west-1a Availability Zone in this scenario.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nSelect the instance that is farthest to the next billing hour."
            },
            {
                "isCorrect": false,
                "text": "​\n\nSelect the instances with the most recent launch configuration."
            },
            {
                "isCorrect": true,
                "text": "​\n\nSelect the instances with the oldest launch configuration.\n\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The default termination policy is designed to help ensure that your network architecture spans Availability Zones evenly. With the default termination policy, the behavior of the Auto Scaling group is as follows:</p><p>1. If there are instances in multiple Availability Zones, choose the Availability Zone with the most instances and at least one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, choose the Availability Zone with the instances that use the oldest launch configuration.</p><p>2. Determine which unprotected instances in the selected Availability Zone use the oldest launch configuration. If there is one such instance, terminate it.</p><p>3. If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) If there is one such instance, terminate it.</p><p>4. If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random.</p><p>The following flow diagram illustrates how the default termination policy works:</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/termination-policy-default-flowchart-diagram.png\"> <br><br></p><p><strong>Reference:</strong><br><br><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p></div>"
    },
    {
        "questionNo": 35,
        "questionText": "You are working as a solutions architect for a large financial company. They have a web application hosted in their on-premises infrastructure which they want to migrate to AWS cloud. Your manager has instructed you to ensure that there is no downtime while the migration process is on-going. In order to achieve this, your team decided to divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure. Once the migration is over and the application works with no issues, a full diversion to AWS will be implemented. The company's VPC is connected to its on-premises network via an AWS Direct Connect connection.\n\nWhich of the following are the possible solutions that you can implement to satisfy the above requirement? (Select TWO.)",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nUse Route 53 with Weighted routing policy to divert the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure.\n\n(Correct)"
            },
            {
                "isCorrect": true,
                "text": "​\n\nUse an Application Elastic Load balancer with Weighted Target Groups to divert and proportion the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse a Network Load balancer with Weighted Target Groups to divert the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure."
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse Route 53 with Failover routing policy to divert and proportion the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure."
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse AWS Global Accelerator to divert and proportion the HTTP and HTTPS traffic between the on-premises and AWS-hosted application. Ensure that the on-premises network has an AnyCast static IP address and is connected to your VPC via a Direct Connect Gateway."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Application Load Balancers support Weighted Target Groups routing. With this feature, you will be able to do weighted routing of the traffic forwarded by a rule to multiple target groups. This enables various use cases like blue-green, canary and hybrid deployments without the need for multiple load balancers. It even enables zero-downtime migration between on-premises and cloud or between different compute types like EC2 and Lambda.</p><p><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2019/10/06/2019-10-06_17-27-58.png\"></p><p>To divert 50% of the traffic to the new application in AWS and the other 50% to the application, you can also use Route 53 with Weighted routing policy. This will divert the traffic between the on-premises and AWS-hosted application accordingly.</p><p>Weighted routing lets you associate multiple resources with a single domain name (tutorialsdojo.com) or subdomain name (portal.tutorialsdojo.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software. You can set a specific percentage of how much traffic will be allocated to the resource by specifying the weights.</p><p>For example, if you want to send a tiny portion of your traffic to one resource and the rest to another resource, you might specify weights of 1 and 255. The resource with a weight of 1 gets 1/256th of the traffic (1/1+255), and the other resource gets 255/256ths (255/1+255).</p><p>You can gradually change the balance by changing the weights. If you want to stop sending traffic to a resource, you can change the weight for that record to 0.</p><p>When you create a target group in your Application Load Balancer, you specify its target type. This determines the type of target you specify when registering with this target group. You can select the following target types:</p><p>1. <strong>instance</strong> - The targets are specified by instance ID.<br>2. <strong>ip</strong> - The targets are IP addresses.<br>3. <strong>Lambda</strong> - The target is a Lambda function.</p><p><img src=\"https://media.amazonwebservices.com/blog/2017/alb_add_ip_targets_2.png\"></p><p>When the target type is <strong>ip</strong>, you can specify IP addresses from one of the following CIDR blocks:<br><br>- <code>10.0.0.0/8</code> (RFC 1918)<br>- <code>100.64.0.0/10</code> (RFC 6598)<br>- <code>172.16.0.0/12</code> (RFC 1918)<br>- <code>192.168.0.0/16</code> (RFC 1918)<br>- The subnets of the VPC for the target group</p><p>These supported CIDR blocks enable you to register the following with a target group: ClassicLink instances, instances in a VPC that is peered to the load balancer VPC, AWS resources that are addressable by IP address and port (for example, databases), and on-premises resources linked to AWS through AWS Direct Connect or a VPN connection.</p><p>Take note that you can not specify publicly routable IP addresses. If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port. Each network interface can have its own security group.</p><p>Hence, the correct answers are the following options:</p><p><strong>- Use an Application Elastic Load balancer with Weighted Target Groups to divert and proportion the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure.</strong></p><p><strong>- Use Route 53 with Weighted routing policy to divert the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure.</strong></p><p>The option that says: <strong>Use a Network Load balancer with Weighted Target Groups to divert the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure</strong> is incorrect because a Network Load balancer doesn't have Weighted Target Groups to divert the traffic between the on-premises and AWS-hosted application.</p><p>The option that says: <strong>Use Route 53 with Failover routing policy to divert and proportion the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure</strong> is incorrect because you cannot divert and proportion the traffic between the on-premises and AWS-hosted application using Route 53 with Failover routing policy. This is primarily used if you want to configure active-passive failover to your application architecture.</p><p>The option that says: <strong>Use AWS Global Accelerator to divert and proportion the HTTP and HTTPS traffic between the on-premises and AWS-hosted application. Ensure that the on-premises network has an AnyCast static IP address and is connected to your VPC via a Direct Connect Gateway</strong> is incorrect because although you can control the proportion of traffic directed to each endpoint using AWS Global Accelerator by assigning weights across the endpoints, it is still wrong to use a Direct Connect Gateway and an AnyCast IP address since these are not required at all. You can only associate static IP addresses provided by AWS Global Accelerator to regional AWS resources or endpoints, such as Network Load Balancers, Application Load Balancers, EC2 Instances, and Elastic IP addresses. Take note that a Direct Connect Gateway, per se, doesn't establish a connection from your on-premises network to your Amazon VPCs. It simply enables you to use your AWS Direct Connect connection to connect to two or more VPCs that are located in different AWS Regions.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/\">https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p></div>"
    },
    {
        "questionNo": 36,
        "questionText": "You have just launched a new API Gateway service which uses AWS Lambda as a serverless computing service. In what type of protocol will your API endpoint be exposed?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nHTTP"
            },
            {
                "isCorrect": true,
                "text": "​\nHTTPS\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nWebSocket"
            },
            {
                "isCorrect": false,
                "text": "​\n\nHTTP/2"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>All of the APIs created with Amazon API Gateway expose <strong>HTTPS</strong> endpoints only. Amazon API Gateway does not support unencrypted (HTTP) endpoints. By default, Amazon API Gateway assigns an internal domain to the API that automatically uses the Amazon API Gateway certificate. When configuring your APIs to run under a custom domain name, you can provide your own certificate for the domain.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href=\"https://aws.amazon.com/api-gateway/faqs/\">https://aws.amazon.com/api-gateway/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon API Gateway Cheat Sheet:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-api-gateway/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-amazon-api-gateway/</span></a></p></div>"
    },
    {
        "questionNo": 37,
        "questionText": "A Solutions Architect is migrating several Windows-based applications to AWS that require a scalable file system storage for high-performance computing (HPC). The storage service must have full support for the SMB protocol and Windows NTFS, Active Directory (AD) integration, and Distributed File System (DFS).\n\nWhich of the following is the MOST suitable storage service that the Architect should use to fulfill this scenario?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nAmazon FSx for Lustre"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAmazon S3 Glacier Deep Archive"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAWS DataSync"
            },
            {
                "isCorrect": true,
                "text": "​\n\nAmazon FSx for Windows File Server\n\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon FSx</strong> provides fully managed third-party file systems. Amazon FSx provides you with the native compatibility of third-party file systems with feature sets for workloads such as Windows-based storage, high-performance computing (HPC), machine learning, and electronic design automation (EDA). You don’t have to worry about managing file servers and storage, as Amazon FSx automates the time-consuming administration tasks such as hardware provisioning, software configuration, patching, and backups. Amazon FSx integrates the file systems with cloud-native AWS services, making them even more useful for a broader set of workloads.</p><p>Amazon FSx provides you with two file systems to choose from: Amazon FSx for Windows File Server for Windows-based applications and Amazon FSx for Lustre for compute-intensive workloads.</p><p><br></p><p><img src=\"https://d1.awsstatic.com/r2018/b/FSx-Windows/FSx_Windows_File_Server_How-it-Works.9396055e727c3903de991e7f3052ec295c86f274.png\"></p><p><br></p><p>For Windows-based applications, Amazon FSx provides fully managed Windows file servers with features and performance optimized for \"lift-and-shift\" business-critical application workloads including home directories (user shares), media workflows, and ERP applications. It is accessible from Windows and Linux instances via the SMB protocol. If you have Linux-based applications, Amazon EFS is a cloud-native fully managed file system that provides simple, scalable, elastic file storage accessible from Linux instances via the NFS protocol.</p><p>For compute-intensive and fast processing workloads, like high-performance computing (HPC), machine learning, EDA, and media processing, Amazon FSx for Lustre, provides a file system that’s optimized for performance, with input and output stored on Amazon S3.</p><p>Hence, the correct answer is: <strong><em>Amazon FSx for Windows File Server</em>.</strong></p><p><strong><em>Amazon S3 Glacier Deep Archive</em></strong> is incorrect because this service is primarily used as a secure, durable, and extremely low-cost cloud storage for data archiving and long-term backup.</p><p><strong><em>AWS DataSync</em></strong> is incorrect because this service simply provides a fast way to move large amounts of data online between on-premises storage and Amazon S3 or Amazon Elastic File System (Amazon EFS).</p><p><strong><em>Amazon FSx for Lustre</em></strong> is incorrect because this service doesn't support the Windows-based applications as well as Windows servers.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/fsx/\">https://aws.amazon.com/fsx/</a></p><p><a href=\"https://aws.amazon.com/getting-started/use-cases/hpc/3/\">https://aws.amazon.com/getting-started/use-cases/hpc/3/</a></p><p><br></p><p><strong>Check out this Amazon FSx Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-fsx/ ?src=udemy\">https://tutorialsdojo.com/amazon-fsx/</a></p></div>"
    },
    {
        "questionNo": 38,
        "questionText": "You are working as a Principal Solutions Architect for a leading digital news company which has both an on-premises data center as well as an AWS cloud infrastructure. They store their graphics, audios, videos, and other multimedia assets primarily in their on-premises storage server and use an S3 Standard storage class bucket as a backup. Their data are heavily used for only a week (7 days) but after that period, it will only be infrequently used by their customers. You are instructed to save storage costs in AWS yet maintain the ability to fetch a subset of their media assets in a matter of minutes for a surprise annual data audit, which will be conducted on their cloud storage.\n\nWhich of the following are valid options that you can implement to meet the above requirement? (Select TWO.)",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nSet a lifecycle policy in the bucket to transition to S3 - Standard IA after 30 days\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nSet a lifecycle policy in the bucket to transition the data to S3 - Standard IA storage class after one week (7 days)."
            },
            {
                "isCorrect": false,
                "text": "​\n\nSet a lifecycle policy in the bucket to transition the data to S3 Glacier Deep Archive storage class after one week (7 days)."
            },
            {
                "isCorrect": false,
                "text": "​\n\nSet a lifecycle policy in the bucket to transition the data to S3 - One Zone-Infrequent Access storage class after one week (7 days)."
            },
            {
                "isCorrect": true,
                "text": "​\n\nSet a lifecycle policy in the bucket to transition the data to Glacier after one week (7 days).\n\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can add rules in a lifecycle configuration to tell Amazon S3 to transition objects to another Amazon S3 storage class. For example: When you know that objects are infrequently accessed, you might transition them to the STANDARD_IA storage class. Or transition your data to the GLACIER storage class in case you want to archive objects that you don't need to access in real time.</p><p>In a lifecycle configuration, you can define rules to transition objects from one storage class to another to save on storage costs. When you don't know the access patterns of your objects or your access patterns are changing over time, you can transition the objects to the INTELLIGENT_TIERING storage class for automatic cost savings.</p><p>The lifecycle storage class transitions have a constraint when you want to transition from the STANDARD storage classes to either STANDARD_IA or ONEZONE_IA. The following constraints apply:</p><p>- For larger objects, there is a cost benefit for transitioning to STANDARD_IA or ONEZONE_IA. Amazon S3 does not transition objects that are smaller than 128 KB to the STANDARD_IA or ONEZONE_IA storage classes because it's not cost effective.</p><p>- Objects must be stored <strong>at least 30 days</strong> in the current storage class before you can transition them to STANDARD_IA or ONEZONE_IA. For example, you cannot create a lifecycle rule to transition objects to the STANDARD_IA storage class one day after you create them. Amazon S3 doesn't transition objects within the first 30 days because newer objects are often accessed more frequently or deleted sooner than is suitable for STANDARD_IA or ONEZONE_IA storage.</p><p>- If you are transitioning noncurrent objects (in versioned buckets), you can transition only objects that are at least 30 days noncurrent to STANDARD_IA or ONEZONE_IA storage.</p><p><br></p><p><img src=\"https://udemy-images.s3.amazonaws.com/redactor/raw/2019-04-02_04-26-21-158b6e1eb7ef7be3b91049e88f056f3a.gif\"></p><p><br></p><p>Since there is a time constraint in transitioning objects in S3, you can only change the storage class of your objects from S3 Standard storage class to STANDARD_IA or ONEZONE_IA storage after 30 days. This limitation does not apply on INTELLIGENT_TIERING, GLACIER, and DEEP_ARCHIVE storage class.</p><p>In addition, the requirement says that the media assets should be fetched in a matter of minutes for a surprise <strong>annual</strong> data audit. This means that the retrieval will only happen once a year. You can use expedited retrievals in Glacier which will allow you to quickly access your data (within 1–5 minutes) when occasional urgent requests for a subset of archives are required.</p><p>In this scenario, you can set a lifecycle policy in the bucket to transition to S3 - Standard IA after 30 days or alternatively, you can directly transition your data to Glacier after one week (7 days).</p><p>Hence, the following are the correct answers:</p><p><strong>- Set a lifecycle policy in the bucket to transition the data from Standard storage class to Glacier after one week (7 days).</strong></p><p><strong>- Set a lifecycle policy in the bucket to transition to S3 - Standard IA after 30 days.</strong></p><p><strong>Setting a lifecycle policy in the bucket to transition the data to S3 - Standard IA storage class after one week (7 days)</strong> and <strong>setting a lifecycle policy in the bucket to transition the data to S3 - One Zone-Infrequent Access storage class after one week (7 days)</strong> are both incorrect because there is a constraint in S3 that objects must be stored at least 30 days in the current storage class before you can transition them to STANDARD_IA or ONEZONE_IA. You cannot create a lifecycle rule to transition objects to either STANDARD_IA or ONEZONE_IA storage class 7 days after you create them because you can only do this after the 30-day period has elapsed. Hence, these options are incorrect.</p><p><strong>Setting a lifecycle policy in the bucket to transition the data to S3 Glacier Deep Archive storage class after one week (7 days)</strong> is incorrect because although DEEP_ARCHIVE storage class provides the most cost-effective storage option, it does not have the ability to do expedited retrievals, unlike Glacier. In the event that the surprise annual data audit happens, it may take several hours before you can retrieve your data.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/restoring-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/restoring-objects.html</a></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</a></p></div>"
    },
    {
        "questionNo": 39,
        "questionText": "A company needs secure access to its Amazon RDS for MySQL database that is used by multiple applications. Each IAM user must use a short-lived authentication token to connect to the database.\n\nWhich of the following is the most suitable solution in this scenario?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nUse IAM DB Authentication and create database accounts using the AWS-provided AWSAuthenticationPlugin plugin in MySQL.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse an MFA token to access and connect to a database."
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse AWS Secrets Manager to generate and store short-lived authentication tokens."
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse AWS SSO to access the RDS database."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance.</p><p>An <strong>authentication token</strong> is a string of characters that you use instead of a password. After you generate an authentication token, it's valid for 15 minutes before it expires. If you try to connect using an expired token, the connection request is denied.</p><p><img src=\"https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-13_07-04-06-a2157247b0fa129795001208504fcb51.png\"></p><p>Since the scenario asks you to create a short-lived authentication token to access an Amazon RDS database, you can use an IAM database authentication when connecting to a database instance. Authentication is handled by <code>AWSAuthenticationPlugin</code>—an AWS-provided plugin that works seamlessly with IAM to authenticate your IAM users.</p><p>IAM database authentication provides the following benefits:</p><p>Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL).</p><p>You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance.</p><p>For applications running on Amazon EC2, you can use profile credentials specific to your EC2 instance to access your database instead of a password, for greater security</p><p>Hence, the correct answer is the option that says:<strong> Use IAM DB Authentication and create database accounts using the AWS-provided </strong><code><strong>AWSAuthenticationPlugin</strong></code><strong> plugin in MySQL.</strong></p><p>The options that say: <strong>Use AWS SSO to access the RDS database </strong>is incorrect because AWS SSO just enables you to centrally manage SSO access and user permissions for all of your AWS accounts managed through AWS Organizations.</p><p>The option that says: <strong>Use AWS Secrets Manager to generate and store short-lived authentication tokens </strong>is incorrect because AWS Secrets Manager is not a suitable service to create an authentication token to access an Amazon RDS database. It's primarily used to store passwords, secrets, and other sensitive credentials. It can't generate a short-lived token either. You have to use IAM DB Authentication instead.</p><p>The option that says:<strong> Use an MFA token to access and connect to a database</strong> is incorrect because you can't use an MFA token to connect to your database. You have to set up IAM DB Authentication instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBAuth.Connecting.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBAuth.Connecting.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.DBAccounts.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.DBAccounts.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/ ?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
    },
    {
        "questionNo": 40,
        "questionText": "A loan processing application is hosted in a single On-Demand EC2 instance in your VPC. To improve the scalability of your application, you have to use Auto Scaling to automatically add new EC2 instances to handle a surge of incoming requests.\n\nWhich of the following items should be done in order to add an existing EC2 instance to an Auto Scaling group? (Select TWO.)",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nYou have to ensure that the instance is launched in one of the Availability Zones defined in your Auto Scaling group.\n\n(Correct)"
            },
            {
                "isCorrect": true,
                "text": "​\nYou have to ensure that the AMI used to launch the instance still exists.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nYou must stop the instance first."
            },
            {
                "isCorrect": false,
                "text": "​\nYou have to ensure that the instance is in a different Availability Zone as the Auto Scaling group."
            },
            {
                "isCorrect": false,
                "text": "​\n\nYou have to ensure that the AMI used to launch the instance no longer exists."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Amazon EC2 Auto Scaling provides you with an option to enable automatic scaling for one or more EC2 instances by attaching them to your existing Auto Scaling group. After the instances are attached, they become a part of the Auto Scaling group.</p><p><img src=\"https://s3.amazonaws.com/chrisb/concept_diagram.jpg\"></p><p>The instance that you want to attach must meet the following criteria:</p><p>- The instance is in the <code><strong>running</strong></code> state.</p><p>- The AMI used to launch the instance must still exist.</p><p>- The instance is not a member of another Auto Scaling group.</p><p>- The instance is launched into one of the Availability Zones defined in your Auto Scaling group.</p><p>- If the Auto Scaling group has an attached load balancer, the instance and the load balancer must both be in EC2-Classic or the same VPC. If the Auto Scaling group has an attached target group, the instance and the load balancer must both be in the same VPC.</p><p><br></p><p>Based on the above criteria, the following are the correct answers among the given options:</p><p><strong><em>- You have to ensure that the AMI used to launch the instance still exists.</em></strong></p><p><strong><em>- You have to ensure that the instance is launched in one of the Availability Zones defined in your Auto Scaling group.</em></strong></p><p>The option that says: <strong><em>You must stop the instance first</em></strong> is incorrect because you can directly add a running EC2 instance to an Auto Scaling group without stopping it.</p><p>The option that says: <strong><em>You have to ensure that the AMI used to launch the instance no longer exists</em></strong><em> </em>is incorrect because it should be the other way around. The AMI used to launch the instance should still exist.</p><p>The option that says: <strong><em>You have to ensure that the instance is in a different Availability Zone as the Auto Scaling group</em></strong> is incorrect because the instance should be launched in one of the Availability Zones defined in your Auto Scaling group.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/autoscaling/latest/userguide/attach-instance-asg.html\">http://docs.aws.amazon.com/autoscaling/latest/userguide/attach-instance-asg.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/\">https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/</a></p></div>"
    },
    {
        "questionNo": 41,
        "questionText": "You just joined a large tech company with an existing Amazon VPC. When reviewing the Auto Scaling events, you noticed that their web application is scaling up and down multiple times within the hour.\n\nWhat design change could you make to optimize cost while preserving elasticity?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nAdd provisioned IOPS to the instances"
            },
            {
                "isCorrect": false,
                "text": "​\nIncrease the instance type in the launch configuration"
            },
            {
                "isCorrect": true,
                "text": "​\n\nChange the cooldown period of the Auto Scaling group and set the CloudWatch metric to a higher threshold\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nIncrease the base number of Auto Scaling instances for the Auto Scaling group"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Since the application is scaling up and down multiple times within the hour, the issue lies on the cooldown period of the Auto Scaling group.</p> <p>The cooldown period is a configurable setting for your Auto Scaling group that helps to ensure that it doesn't launch or terminate additional instances before the previous scaling activity takes effect. After the Auto Scaling group dynamically scales using a simple scaling policy, it waits for the cooldown period to complete before resuming scaling activities.</p> <p>When you manually scale your Auto Scaling group, the default is not to wait for the cooldown period, but you can override the default and honor the cooldown period. If an instance becomes unhealthy, the Auto Scaling group does not wait for the cooldown period to complete before replacing the unhealthy instance.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href=\"http://docs.aws.amazon.com/autoscaling/latest/userguide/as-scale-based-on-demand.html\">http://docs.aws.amazon.com/autoscaling/latest/userguide/as-scale-based-on-demand.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p></div>"
    },
    {
        "questionNo": 42,
        "questionText": "An application is hosted on an EC2 instance with multiple EBS Volumes attached and uses Amazon Neptune as its database. To improve data security, you encrypted all of the EBS volumes attached to the instance to protect the confidential data stored in the volumes. \n\nWhich of the following statements are true about encrypted Amazon Elastic Block Store volumes? (Select TWO.)",
        "options": [
            {
                "isCorrect": true,
                "text": "​\nSnapshots are automatically encrypted.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nOnly the data in the volume is encrypted and not all the data moving between the volume and the instance."
            },
            {
                "isCorrect": false,
                "text": "​\nSnapshots are not automatically encrypted."
            },
            {
                "isCorrect": true,
                "text": "​\nAll data moving between the volume and the instance are encrypted.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nThe volumes created from the encrypted snapshot are not encrypted."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Elastic Block Store (Amazon EBS)</strong> provides block level storage volumes for use with EC2 instances. EBS volumes are highly available and reliable storage volumes that can be attached to any running instance that is in the same Availability Zone. EBS volumes that are attached to an EC2 instance are exposed as storage volumes that persist independently from the life of the instance.</p><p><img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/architecture_storage.png\"></p><p><br></p><p>When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:</p><p>- Data at rest inside the volume</p><p>- All data moving between the volume and the instance</p><p>- All snapshots created from the volume</p><p>- All volumes created from those snapshots</p><p>Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage. You can encrypt both the boot and data volumes of an EC2 instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html\">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><br></p><p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-ebs/?src=udemy\">https://tutorialsdojo.com/amazon-ebs/</a></p></div>"
    },
    {
        "questionNo": 43,
        "questionText": "A financial company wants to store their data in Amazon S3 but at the same time, they want to store their frequently accessed data locally on their on-premises server. This is due to the fact that they do not have the option to extend their on-premises storage, which is why they are looking for a durable and scalable storage service to use in AWS.\n\nWhat is the best solution for this scenario?",
        "options": [
            {
                "isCorrect": true,
                "text": "​\nUse the Amazon Storage Gateway - Cached Volumes.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nUse Amazon Glacier."
            },
            {
                "isCorrect": false,
                "text": "​\nUse a fleet of EC2 instance with EBS volumes to store the commonly used data."
            },
            {
                "isCorrect": false,
                "text": "​\nUse both Elasticache and S3 for frequently accessed data."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>By using Cached volumes, you store your data in Amazon Simple Storage Service (Amazon S3) and retain a copy of frequently accessed data subsets locally in your on-premises network. Cached volumes offer substantial cost savings on primary storage and minimize the need to scale your storage on-premises. You also retain low-latency access to your frequently accessed data. This is the best solution for this scenario.</p><p><strong>Using a fleet of EC2 instance with EBS volumes to store the commonly used data</strong> is incorrect because an EC2 instance is not a storage service and it does not provide the required durability and scalability.</p><p><strong>Using both Elasticache and S3 for frequently accessed data</strong> is incorrect as this is not efficient. Moreover, the question explicitly said that the frequently accessed data should be stored locally on their on-premises server and not on AWS.</p><p><strong>Using Amazon Glacier</strong> is incorrect as this is mainly used for data archiving.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/storagegateway/faqs/\">https://aws.amazon.com/storagegateway/faqs/</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-aws-storage-gateway/\">https://tutorialsdojo.com/aws-cheat-sheet-aws-storage-gateway/</a></p></div>"
    },
    {
        "questionNo": 44,
        "questionText": "A company has 10 TB of infrequently accessed financial data files that would need to be stored in AWS. These data would be accessed infrequently during specific weeks when they are retrieved for auditing purposes. The retrieval time is not strict as long as it does not exceed 24 hours.\n\nWhich of the following would be a secure, durable, and cost-effective solution for this scenario?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nUpload the data to S3 then use a lifecycle policy to transfer data to S3 One Zone-IA."
            },
            {
                "isCorrect": true,
                "text": "​\n\nUpload the data to S3 and set a lifecycle policy to transition data to Glacier after 0 days.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nUpload the data to Amazon FSx for Windows File Server using the Server Message Block (SMB) protocol."
            },
            {
                "isCorrect": false,
                "text": "​\n\nUpload the data to S3 then use a lifecycle policy to transfer data to S3-IA."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Glacier is a cost-effective archival solution for large amounts of data. Bulk retrievals are S3 Glacier’s lowest-cost retrieval option, enabling you to retrieve large amounts, even petabytes, of data inexpensively in a day. Bulk retrievals typically complete within 5 – 12 hours. You can specify an absolute or relative time period (including 0 days) after which the specified Amazon S3 objects should be transitioned to Amazon Glacier.</p><p>Hence, the correct answer is the option that says: <strong>Upload the data to S3 and set a lifecycle policy to transition data to Glacier after </strong><code><strong>0</strong></code><strong> days.</strong></p><p>Glacier has a management console that you can use to create and delete vaults. However, you cannot directly upload archives to Glacier by using the management console. To upload data such as photos, videos, and other documents, you must either use the AWS CLI or write code to make requests by using either the REST API directly or by using the AWS SDKs.</p><p>Take note that uploading data to the S3 Console and setting its storage class of \"Glacier\" is a different story as the proper way to upload data to Glacier is still via its API or CLI. In this way, you can set up your vaults and configure your retrieval options. If you uploaded your data using the S3 console then it will be managed via S3 even though it is internally using a Glacier storage class.</p><p><img src=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/images/lifecycle-config-transition.png\"></p><p><strong>Uploading the data to S3 then using a lifecycle policy to transfer data to S3-IA</strong> is incorrect because using Glacier would be a more cost-effective solution than using S3-IA. Since the required retrieval period should not exceed more than a day, Glacier would be the best choice.</p><p><strong>Uploading the data to Amazon FSx for Windows File Server using the Server Message Block (SMB) protocol</strong> is incorrect because this option costs more than Amazon Glacier, which is more suitable for storing infrequently accessed data. Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol.</p><p><strong>Uploading the data to S3 then using a lifecycle policy to transfer data to S3 One Zone-IA</strong> is incorrect because with S3 One Zone-IA, the data will only be stored in a single availability zone and thus, this storage solution is not durable. It also costs more compared to Glacier.</p><p><br><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/glacier/faqs/\">https://aws.amazon.com/glacier/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Glacier Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-glacier/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-glacier/</a></p><p><br></p><p><strong>Here is a deep dive on Amazon S3 and Glacier Best Practices:</strong></p><p><a href=\"https://youtu.be/rHeTn9pHNKo\">https://youtu.be/rHeTn9pHNKo</a></p></div>"
    },
    {
        "questionNo": 45,
        "questionText": "A leading media company has recently adopted a hybrid cloud architecture which requires them to migrate their application servers and databases in AWS. One of their applications requires a heterogeneous database migration in which you need to transform your on-premises Oracle database to PostgreSQL in AWS. This entails a schema and code transformation before the proper data migration starts.   \n\nWhich of the following options is the most suitable approach to migrate the database in AWS? ",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nConfigure a Launch Template that automatically converts the source schema and code to match that of the target database. Then, use the AWS Database Migration Service to migrate data from the source database to the target database. "
            },
            {
                "isCorrect": true,
                "text": "​\n\nFirst, use the AWS Schema Conversion Tool to convert the source schema and application code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nHeterogeneous database migration is not supported in AWS. You have to transform your database first to PostgreSQL and then migrate it to RDS. "
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse Amazon Neptune to convert the source schema and code to match that of the target database in RDS. Use the AWS Batch to effectively migrate the data from the source database to the target database in a batch process. "
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Database Migration Service</strong> helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from most widely used commercial and open-source databases.</p><p>AWS Database Migration Service can migrate your data to and from most of the widely used commercial and open source databases. It supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle to Amazon Aurora. Migrations can be from on-premises databases to Amazon RDS or Amazon EC2, databases running on EC2 to RDS, or vice versa, as well as from one RDS database to another RDS database. It can also move data between SQL, NoSQL, and text based targets.</p><p>In heterogeneous database migrations the source and target databases engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts. That makes heterogeneous migrations a two step process. First use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database. All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located in your own premises outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS.</p><p>The option that says: <strong>Configure a Launch Template that automatically converts the source schema and code to match that of the target database. Then, use the AWS Database Migration Service to migrate data from the source database to the target database</strong> is incorrect because Launch templates are primarily used in EC2 to enable you to store launch parameters so that you do not have to specify them every time you launch an instance.</p><p>The option that says: <strong>Use Amazon Neptune to convert the source schema and code to match that of the target database in RDS. Use the AWS Batch to effectively migrate the data from the source database to the target database in a batch process</strong> is incorrect because Amazon Neptune is a fully-managed graph database service and not a suitable service to use to convert the source schema. AWS Batch is not a database migration service and hence, it is not suitable to be used in this scenario. You should use the AWS Schema Conversion Tool and AWS Database Migration Service instead.</p><p>The option that says: <strong>Heterogeneous database migration is not supported in AWS. You have to transform your database first to PostgreSQL and then migrate it to RDS</strong> is incorrect because heterogeneous database migration is supported in AWS using the Database Migration Service.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html</a></p><p><a href=\"https://aws.amazon.com/batch/\">https://aws.amazon.com/batch/</a></p><p><br></p><p><strong>Check out this AWS Database Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-database-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-database-migration-service/</a></p><p><br></p><p><strong>Here is a case study on AWS Database Migration Service:</strong></p><p><a href=\"https://youtu.be/11IHvxjy4hw\">https://youtu.be/11IHvxjy4hw</a></p></div>"
    },
    {
        "questionNo": 46,
        "questionText": "You are working as an IT Consultant for a large financial firm. They have a requirement to store irreproducible financial documents using Amazon S3. For their quarterly reporting, the files are required to be retrieved after a period of 3 months. There will be some occasions when a surprise audit will be held, which requires access to the archived data that they need to present immediately.\n\nWhat will you do to satisfy this requirement in a cost-effective way?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nUse Amazon S3 -Intelligent Tiering"
            },
            {
                "isCorrect": false,
                "text": "​\nUse Amazon S3 Standard"
            },
            {
                "isCorrect": true,
                "text": "​\nUse Amazon S3 Standard - Infrequent Access\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse Amazon Glacier Deep Archive"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the requirement is to have a storage option that is cost-effective and has the ability to access or retrieve the archived data immediately. The cost-effective options are Amazon Glacier Deep Archive and Amazon S3 Standard- Infrequent Access (Standard - IA). However, the former option is not designed for rapid retrieval of data which is required for the surprise audit. Hence, <strong>using Amazon Glacier Deep Archive</strong> is incorrect and the best answer is to <strong>use Amazon S3 Standard - Infrequent Access</strong>.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/images/ObjectStorageClass.png\"></p><p><br></p><p><strong>Using Amazon S3 Standard</strong> is incorrect because the standard storage class is not cost-efficient in this scenario. It costs more than Glacier Deep Archive and S3 Standard - Infrequent Access.</p><p><strong>Using Amazon S3 -Intelligent Tiering</strong> is incorrect because the Intelligent Tiering storage class entails an additional fee for monitoring and automation of each object in your S3 bucket vs. the Standard storage class and S3 Standard - Infrequent Access.</p><p>Amazon S3 Standard - Infrequent Access is an Amazon S3 storage class for data that is accessed less frequently, but requires rapid access when needed. Standard - IA offers the high durability, throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee.</p><p>This combination of low cost and high performance makes Standard - IA ideal for long-term storage, backups, and as a data store for disaster recovery. The Standard - IA storage class is set at the object level and can exist in the same bucket as Standard, allowing you to use lifecycle policies to automatically transition objects between storage classes without any application changes.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
    },
    {
        "questionNo": 47,
        "questionText": "You are an IT Consultant for a top investment bank which is in the process of building its new Forex trading platform. To ensure high availability and scalability, you designed the trading platform to use an Elastic Load Balancer in front of an Auto Scaling group of On-Demand EC2 instances across multiple Availability Zones. For its database tier, you chose to use a single Amazon Aurora instance to take advantage of its distributed, fault-tolerant and self-healing storage system. \n\nIn the event of system failure on the primary database instance, what happens to Amazon Aurora during the failover?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nAmazon Aurora flips the A record of your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary."
            },
            {
                "isCorrect": true,
                "text": "​\n\nAurora will attempt to create a new DB Instance in the same Availability Zone as the original instance and is done on a best-effort basis.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAurora will first attempt to create a new DB Instance in a different Availability Zone of the original instance. If unable to do so, Aurora will attempt to create a new DB Instance in the original Availability Zone in which the instance was first launched."
            },
            {
                "isCorrect": false,
                "text": "​\n\nAmazon Aurora flips the canonical name record (CNAME) for your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Failover is automatically handled by Amazon Aurora so that your applications can resume database operations as quickly as possible without manual administrative intervention.</p><p><img src=\"https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2018/06/08/Aurora-Arch.jpg\"></p><p>If you have an Amazon Aurora Replica in the same or a different Availability Zone, when failing over, Amazon Aurora flips the canonical name record (CNAME) for your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary. Start-to-finish, failover typically completes within 30 seconds.</p><p>If you are running Aurora Serverless and the DB instance or AZ become unavailable, Aurora will automatically recreate the DB instance in a different AZ.</p><p>If you do not have an Amazon Aurora Replica (i.e. single instance) and are not running Aurora Serverless, Aurora will attempt to create a new DB Instance in the same Availability Zone as the original instance. This replacement of the original instance is done on a best-effort basis and may not succeed, for example, if there is an issue that is broadly affecting the Availability Zone.</p><p>Hence, the correct answer is the option that says:<strong> Aurora will attempt to create a new DB Instance in the same Availability Zone as the original instance and is done on a best-effort basis.</strong></p><p>The options that say: <strong>Amazon Aurora flips the canonical name record (CNAME) for your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary</strong> and <strong>Amazon Aurora flips the A record of your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary</strong> are incorrect because this will only happen if you are using an Amazon Aurora Replica. In addition, Amazon Aurora flips the canonical name record (CNAME) and not the A record (IP address) of the instance.</p><p>The option that says: <strong>Aurora will first attempt to create a new DB Instance in a different Availability Zone of the original instance. If unable to do so, Aurora will attempt to create a new DB Instance in the original Availability Zone in which the instance was first launched</strong> is incorrect because Aurora will first attempt to create a new DB Instance in the same Availability Zone as the original instance. If unable to do so, Aurora will attempt to create a new DB Instance in a different Availability Zone and not the other way around.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/faqs/\">https://aws.amazon.com/rds/aurora/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html</a></p><p><br></p><p><strong>Check out this Amazon Aurora Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-aurora/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-aurora/</a></p></div>"
    },
    {
        "questionNo": 48,
        "questionText": "An organization plans to run an application in a dedicated physical server that doesn’t use virtualization. The application data will be stored in a storage solution that uses an NFS protocol. To prevent data loss, you need to use a durable cloud storage service to store a copy of your data.\n\nWhich of the following is the most suitable solution to meet the requirement?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nUse an AWS Storage Gateway hardware appliance for your compute resources. Configure Volume Gateway to store the application data and backup data."
            },
            {
                "isCorrect": true,
                "text": "​\n\nUse an AWS Storage Gateway hardware appliance for your compute resources. Configure File Gateway to store the application data and create an Amazon S3 bucket to store a backup of your data.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse AWS Storage Gateway with a gateway VM appliance for your compute resources. Configure File Gateway to store the application data and backup data."
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse an AWS Storage Gateway hardware appliance for your compute resources. Configure Volume Gateway to store the application data and create an Amazon S3 bucket to store a backup of your data."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Storage Gateway</strong> is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage by linking it to S3. Storage Gateway provides 3 types of storage solutions for your on-premises applications: file, volume, and tape gateways. The AWS Storage Gateway Hardware Appliance is a physical, standalone, validated server configuration for on-premises deployments.</p><p><img src=\"https://d1.awsstatic.com/cloud-storage/File-Gateway-How-it-Works.6a5ce3c54688864e5b951df9cb8732fc4f2926b4.png\"></p><p>The AWS Storage Gateway Hardware Appliance is a physical hardware appliance with the Storage Gateway software preinstalled on a validated server configuration. The hardware appliance is a high-performance 1U server that you can deploy in your data center, or on-premises inside your corporate firewall. When you buy and activate your hardware appliance, the activation process associates your hardware appliance with your AWS account. After activation, your hardware appliance appears in the console as a gateway on the <em>Hardware</em> page. You can configure your hardware appliance as a file gateway, tape gateway, or volume gateway type. The procedure that you use to deploy and activate these gateway types on a hardware appliance is the same as on a virtual platform.</p><p>Since the company needs to run a dedicated physical appliance, you can use an AWS Storage Gateway Hardware Appliance. It comes pre-loaded with Storage Gateway software, and provides all the required resources to create a file gateway. A file gateway can be configured to store and retrieve objects in Amazon S3 using the protocols NFS and SMB.</p><p>Hence, the correct answer in this scenario is: <strong>Use an AWS Storage Gateway hardware appliance for your compute resources. Configure File Gateway to store the application data and create an Amazon S3 bucket to store a backup of your data</strong>.</p><p>The option that says: <strong>Use AWS Storage Gateway with a gateway VM appliance for your compute resources. Configure File Gateway to store the application data and backup data</strong> is incorrect because the scenario says that the company needs to use an on-premises hardware appliance and not just a Virtual Machine (VM).</p><p>The options that say: <strong>Use an AWS Storage Gateway hardware appliance for your compute resources. Configure Volume Gateway to store the application data and backup data </strong>and <strong>Use an AWS Storage Gateway hardware appliance for your compute resources. Configure Volume Gateway to store the application data and create an Amazon S3 bucket to store a backup of your data</strong> are both incorrect because the scenario requires a file system that uses an NFS protocol and not iSCSI devices. Among the AWS Storage Gateway storage solutions, only file gateway can store and retrieve objects in Amazon S3 using the protocols NFS and SMB.<br></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/hardware-appliance.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/hardware-appliance.html</a></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p></div>"
    },
    {
        "questionNo": 49,
        "questionText": "A company has recently adopted a hybrid cloud architecture and is planning to migrate a database hosted on-premises to AWS. The database currently has over 50 TB of consumer data, handles highly transactional (OLTP) workloads, and is expected to grow. The Solutions Architect should ensure that the database is ACID-compliant and can handle complex queries of the application.\n\nWhich type of database service should the Architect use?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nAmazon RDS"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAmazon Redshift"
            },
            {
                "isCorrect": true,
                "text": "​\n\nAmazon Aurora\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAmazon DynamoDB"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Aurora (Aurora)</strong> is a fully managed relational database engine that's compatible with MySQL and PostgreSQL. You already know how MySQL and PostgreSQL combine the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases. The code, tools, and applications you use today with your existing MySQL and PostgreSQL databases can be used with Aurora. With some workloads, Aurora can deliver up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.</p><p>Aurora includes a high-performance storage subsystem. Its MySQL- and PostgreSQL-compatible database engines are customized to take advantage of that fast distributed storage. The underlying storage grows automatically as needed, up to 64 tebibytes (TiB). Aurora also automates and standardizes database clustering and replication, which are typically among the most challenging aspects of database configuration and administration.</p><p><img src=\"https://udemy-images.s3.amazonaws.com/redactor/raw/2020-01-04_04-04-51-474a73beda1fc209d60e63ed8d226b6a.png\"></p><p>For Amazon RDS MariaDB DB instances, the maximum provisioned storage limit constrains the size of a table to a maximum size of 64 TB when using InnoDB file-per-table tablespaces. This limit also constrains the system tablespace to a maximum size of 16 TB. InnoDB file-per-table tablespaces (with tables each in their own tablespace) is set by default for Amazon RDS MariaDB DB instances.</p><p>Hence, the correct answer is <strong>Amazon Aurora</strong>.</p><p><strong>Amazon Redshift</strong> is incorrect because this is primarily used for OLAP applications and not for OLTP. Moreover, it doesn't scale automatically to handle the exponential growth of the database.</p><p><strong>Amazon DynamoDB</strong> is incorrect. Although you can use this to have an ACID-compliant database, it is not capable of handling complex queries and highly transactional (OLTP) workloads.</p><p><strong>Amazon RDS</strong> is incorrect. Although this service can host an ACID-compliant relational database that can handle complex queries and transactional (OLTP) workloads, it is still not scalable to handle the growth of the database. Amazon Aurora is the better choice as its underlying storage can grow automatically as needed.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/\">https://aws.amazon.com/rds/aurora/</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html</a></p><p><a href=\"https://aws.amazon.com/nosql/\">https://aws.amazon.com/nosql/</a></p><p><br></p><p><strong>Check out this Amazon Aurora Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-aurora/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-aurora/</a></p></div>"
    },
    {
        "questionNo": 50,
        "questionText": "You are designing an online banking application which needs to have a distributed session data management. Currently, the application is hosted on an Auto Scaling group of On-Demand EC2 instances across multiple Availability Zones with a Classic Load Balancer that distributes the load.\n\nWhich of the following options should you do to satisfy the given requirement?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nUse the GetSessionToken action in AWS STS for session management"
            },
            {
                "isCorrect": false,
                "text": "​\n\nSet up an AWS Systems Manager Session Manager"
            },
            {
                "isCorrect": true,
                "text": "​\n\nUse Amazon ElastiCache\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nEnable the sticky session feature in the Classic Load Balancer"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this question, the keyword is <strong>distributed </strong>session data management.</p><p>Sticky session feature of the Classic Load Balancer can also provide session management, however, take note that this feature has its limitations such as, in the event of a failure, you are likely to lose the sessions that were resident on the failed node. In the event that the number of your web servers change when your Auto Scaling kicks in, it’s possible that the traffic may be unequally spread across the web servers as active sessions may exist on particular servers. If not mitigated properly, this can hinder the scalability of your applications. Hence, sticky session is not scalable or \"<strong>distributed</strong>\" as compared with ElastiCache.</p><p>You can manage HTTP session data from the web servers using an In-Memory Key/Value store such as Redis and Memcached. Redis is an open source, in-memory data structure store used as a database, cache, and message broker. Memcached is an in-memory key-value store for small arbitrary data (strings, objects) from results of database calls, API calls, or page rendering.</p><p><br></p><p><img src=\"https://d1.awsstatic.com/product-marketing/caching-session-management-diagram-v2.c6856e6de83c4222dbc4853d9ff873f5542a86d8.PNG\"></p><p><br></p><p>In AWS, you can use Amazon ElastiCache which offers fully managed Redis and Memcached service to manage and store session data for your web applications.</p><p><strong>Setting up an AWS Systems Manager Session Manager</strong> is incorrect because the Session Manager is simply a capability that lets you manage your Amazon EC2 instances through an interactive one-click browser-based shell or through the AWS CLI. This does not act as a distributed session data management.</p><p><strong>Enabling the sticky session feature in the Classic Load Balancer</strong> is incorrect because although you can use this to manage your session data, it is not a \"distributed\" solution compared to ElastiCache.</p><p><strong>Using the </strong><code><strong>GetSessionToken</strong></code><strong> action in AWS STS for session management</strong> is incorrect because <code>GetSessionToken</code> is just one of the available actions in STS which returns a set of temporary credentials for an AWS account or IAM user. This is not used for distributed session data management</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/caching/session-management/\">https://aws.amazon.com/caching/session-management/</a></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><br></p><p><strong>Check out this Amazon ElastiCache Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p><p><br></p><p><strong>Redis (cluster mode enabled vs disabled) vs Memcached:</strong></p><p><a href=\"https://tutorialsdojo.com/redis-cluster-mode-enabled-vs-disabled-vs-memcached/?src=udemy\">https://tutorialsdojo.com/redis-cluster-mode-enabled-vs-disabled-vs-memcached/</a></p></div>"
    },
    {
        "questionNo": 51,
        "questionText": "You are setting up a configuration management in your existing cloud architecture where you have to deploy and manage your EC2 instances including the other AWS resources using Chef and Puppet. Which of the following is the most suitable service to use in this scenario?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nAWS CloudFormation"
            },
            {
                "isCorrect": true,
                "text": "​\nAWS OpsWorks\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAWS CodeDeploy"
            },
            {
                "isCorrect": false,
                "text": "​\nAWS Elastic Beanstalk"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/opsworks/\">https://aws.amazon.com/opsworks/</a></p><p><br></p><p><strong>Check out this AWS OpsWorks Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-opsworks/?src=udemy\">https://tutorialsdojo.com/aws-opsworks/</a></p><p><br></p><p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p><p><a href=\"https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/?src=udemy\">https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/\">https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/</a></p></div>"
    },
    {
        "questionNo": 52,
        "questionText": "You are a Solutions Architect working for a startup which is currently migrating their production environment to AWS. Your manager asked you to set up access to the AWS console using Identity Access Management (IAM). Using the AWS CLI, you have created 5 users for your systems administrators.\n\nWhat further steps do you need to take for your systems administrators to get access to the AWS console?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nProvide the system administrators the secret access key and access key id."
            },
            {
                "isCorrect": false,
                "text": "​\nEnable multi-factor authentication on their accounts and define a password policy."
            },
            {
                "isCorrect": true,
                "text": "​\nProvide a password for each user created and give these passwords to your system administrators.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nAdd the administrators to the Security Group."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The AWS Management Console is the web interface used to manage your AWS resources using your web browser. To access this, your users should have a password that they can use to login to the web console.</p><p><strong>Providing the system administrators the secret access key and access key id</strong> is incorrect as these are used to trigger AWS API calls.</p><p><strong>Enabling multi-factor authentication on their accounts and defining a password policy</strong> is incorrect because the multi-factor authentication and a password policy are just additional security measures for the IAM user but these won't enable them to access the AWS Management Console.</p><p><strong>Adding the administrators to the Security Group</strong> is incorrect as you can't add an IAM user to a security group. Remember that a security group is primarily used for EC2 instances, and not for IAM.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_how-users-sign-in.html\">http://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_how-users-sign-in.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/\">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</a></p></div>"
    },
    {
        "questionNo": 53,
        "questionText": "You are required to deploy a Docker-based batch application to your VPC in AWS. The application will be used to process both mission-critical data as well as non-essential batch jobs. Which of the following is the most cost-effective option to use in implementing this architecture? ",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nUse ECS as the container management service then set up a combination of Reserved and Spot EC2 Instances for processing mission-critical and non-essential batch jobs respectively. \n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse ECS as the container management service then set up Reserved EC2 Instances for processing both mission-critical and non-essential batch jobs. "
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse ECS as the container management service then set up Spot EC2 Instances for processing both mission-critical and non-essential batch jobs. "
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse ECS as the container management service then set up On-Demand EC2 Instances for processing both mission-critical and non-essential batch jobs. "
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon ECS</strong> lets you run batch workloads with managed or custom schedulers on Amazon EC2 On-Demand Instances, Reserved Instances, or Spot Instances. You can launch a combination of EC2 instances to set up a cost-effective architecture depending on your workload. You can launch Reserved EC2 instances to process the mission-critical data and Spot EC2 instances for processing non-essential batch jobs.</p><p>There are two different charge models for Amazon Elastic Container Service (ECS): Fargate Launch Type Model and EC2 Launch Type Model. With Fargate, you pay for the amount of vCPU and memory resources that your containerized application requests while for EC2 launch type model, there is no additional charge. You pay for AWS resources (e.g. EC2 instances or EBS volumes) you create to store and run your application. You only pay for what you use, as you use it; there are no minimum fees and no upfront commitments.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/images/overview-containeragent-fargate.png\"></p><p><br></p><p>In this scenario, the most cost-effective solution is to use ECS as the container management service then set up a combination of Reserved and Spot EC2 Instances for processing mission-critical and non-essential batch jobs respectively. You can use Scheduled Reserved Instances (Scheduled Instances) which enables you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. This will ensure that you have an uninterrupted compute capacity to process your mission-critical batch jobs.</p><p>Hence, the correct answer is the option that says: <strong>Use ECS as the container management service then set up a combination of Reserved and Spot EC2 Instances for processing mission-critical and non-essential batch jobs respectively</strong>.</p><p><strong>Using ECS as the container management service then setting up Reserved EC2 Instances for processing both mission-critical and non-essential batch jobs</strong> is incorrect because processing the non-essential batch jobs can be handled much cheaper by using Spot EC2 instances instead of Reserved Instances.</p><p><strong>Using ECS as the container management service then setting up On-Demand EC2 Instances for processing both mission-critical and non-essential batch jobs</strong> is incorrect because an On-Demand instance costs more compared to Reserved and Spot EC2 instances. Processing the non-essential batch jobs can be handled much cheaper by using Spot EC2 instances instead of On-Demand instances.</p><p><strong>Using ECS as the container management service then setting up Spot EC2 Instances for processing both mission-critical and non-essential batch jobs</strong> is incorrect because although this set up provides the cheapest solution among other options, it will not be able to meet the required workload. Using Spot instances to process mission-critical workloads is not suitable since these types of instances can be terminated by AWS at any time, which can affect critical processing.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p><p><a href=\"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/\">https://aws.amazon.com/ec2/spot/containers-for-less/get-started/</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-container-service-amazon-ecs/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-container-service-amazon-ecs/</a></p></div>"
    },
    {
        "questionNo": 54,
        "questionText": "A web application is hosted on an EC2 instance that processes sensitive financial information which is launched in a private subnet. All of the data are stored in an Amazon S3 bucket. The financial information is accessed by users over the Internet. The security team of the company is concerned that the Internet connectivity to Amazon S3 is a security risk.\n\nIn this scenario, what will you do to resolve this security vulnerability?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nChange the web architecture to access the financial data in S3 through an interface VPC endpoint, which is powered by AWS PrivateLink."
            },
            {
                "isCorrect": false,
                "text": "​\n\nChange the web architecture to access the financial data in your S3 bucket through a VPN connection."
            },
            {
                "isCorrect": false,
                "text": "​\n\nChange the web architecture to access the financial data hosted in your S3 bucket by creating a custom VPC endpoint service."
            },
            {
                "isCorrect": true,
                "text": "​\n\nChange the web architecture to access the financial data through a Gateway VPC Endpoint.\n\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Take note that your VPC lives within a larger AWS network and the services, such as S3, DynamoDB, RDS and many others, are located outside of your VPC, but still within the AWS network. By default, the connection that your VPC uses to connect to your S3 bucket or any other service traverses the public Internet via your Internet Gateway.</p><p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p><p>There are two types of VPC endpoints: <em>interface endpoints</em> and <em>gateway endpoints</em>. You have to create the type of VPC endpoint required by the supported service.</p><p>An <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html\">interface endpoint</a> is an elastic network interface with a private IP address that serves as an entry point for traffic destined to a supported service. A <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">gateway endpoint</a> is a gateway that is a target for a specified route in your route table, used for traffic destined to a supported AWS service. It is important to note that for Amazon S3 and DynamoDB service, you have to create a gateway endpoint and then use an interface endpoint for other services.</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/vpc-endpoint-s3-diagram.png\"></p><p><br></p><p><strong>Changing the web architecture to access the financial data in your S3 bucket through a VPN connection</strong> is incorrect because a VPN connection still goes through the public Internet. You have to use a VPC Endpoint in this scenario and not VPN, to privately connect your VPC to supported AWS services such as S3.</p><p><strong>Changing the web architecture to access the financial data hosted in your S3 bucket by creating a custom VPC endpoint service</strong> is incorrect because a \"VPC endpoint <strong>service</strong>\" is quite different from a \"VPC endpoint\". With VPC endpoint service, you are the service provider where you can create your own application in your VPC and configure it as an AWS PrivateLink-powered service (referred to as an endpoint service). Other AWS principals can create a connection from their VPC to your endpoint service using an interface VPC endpoint.</p><p><strong>Changing the web architecture to access the financial data in S3 through an interface VPC endpoint, which is powered by AWS PrivateLink</strong> is incorrect because although you are correctly using a VPC Endpoint to satisfy the requirement, you chose a wrong type of VPC Endpoint. Remember that for S3 and DynamoDB service, you have to use a <strong>Gateway</strong> VPC Endpoint and not an <strong>Interface</strong> VPC Endpoint.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></p></div>"
    },
    {
        "questionNo": 55,
        "questionText": "A website is running on an Auto Scaling group of On-Demand EC2 instances which are abruptly getting terminated from time to time. To automate the monitoring process, you started to create a simple script which uses the AWS CLI to find the root cause of this issue. \n\nWhich of the following is the most suitable command to use?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\naws ec2 describe-volume-status"
            },
            {
                "isCorrect": true,
                "text": "​\n\naws ec2 describe-instances\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\naws ec2 describe-images"
            },
            {
                "isCorrect": false,
                "text": "​\n\naws ec2 get-console-screenshot"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The <code>describe-instances</code> command shows the status of the EC2 instances including the recently terminated instances. It also returns a <code>StateReason</code> of why the instance was terminated.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href=\"http://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html\">http://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p></div>"
    },
    {
        "questionNo": 56,
        "questionText": "You are working as a Solutions Architect for a leading technology company where you are instructed to troubleshoot the operational issues of your cloud architecture by logging the AWS API call history of your AWS resources. You need to quickly identify the most recent changes made to resources in your environment, including creation, modification, and deletion of AWS resources. One of the requirements is that the generated log files should be encrypted to avoid any security issues.   \n\nWhich of the following is the most suitable approach to implement the encryption?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nUse CloudTrail and configure the destination Amazon Glacier archive to use Server-Side Encryption (SSE)."
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse CloudTrail and configure the destination S3 bucket to use Server Side Encryption (SSE) with AES-128 encryption algorithm."
            },
            {
                "isCorrect": true,
                "text": "​\n\nUse CloudTrail with its default settings\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nUse CloudTrail and configure the destination S3 bucket to use Server-Side Encryption (SSE)."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>By default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE). You can also choose to encrypt your log files with an AWS Key Management Service (AWS KMS) key. You can store your log files in your bucket for as long as you want. You can also define Amazon S3 lifecycle rules to archive or delete log files automatically. If you want notifications about log file delivery and validation, you can set up Amazon SNS notifications.</p><p><img src=\"https://media.amazonwebservices.com/blog/2014/cloudtrail_flow_9.png\"></p><p><strong>Using CloudTrail and configuring the destination Amazon Glacier archive to use Server-Side Encryption (SSE)</strong> is incorrect because CloudTrail stores the log files to S3 and not in Glacier. Take note that by default,&nbsp;CloudTrail event log files are already encrypted using Amazon S3 server-side encryption (SSE).</p><p><strong>Using CloudTrail and configuring the destination S3 bucket to use Server-Side Encryption (SSE)</strong> is incorrect because&nbsp;CloudTrail event log files are already encrypted using the Amazon S3 server-side encryption (SSE) which is why you do not have to do this anymore.</p><p><strong>Use CloudTrail and configure the destination S3 bucket to use Server Side Encryption (SSE) with AES-128 encryption algorithm</strong>&nbsp;is incorrect because Cloudtrail event log files are already encrypted using the Amazon S3 server-side encryption (SSE) by default. Additionally, SSE-S3 only uses the AES-256 encryption algorithm and not the AES-128.</p><p>&nbsp;</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/category/cloud-trail/\">https://aws.amazon.com/blogs/aws/category/cloud-trail/</a></p><p>&nbsp;&nbsp;</p><p><strong>Check out this&nbsp;AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/ ?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p></div>"
    },
    {
        "questionNo": 57,
        "questionText": "A company has an application hosted in an Amazon ECS Cluster behind an Application Load Balancer. The Solutions Architect is building a sophisticated web filtering solution that allows or blocks web requests based on the country that the requests originate from. However, the solution should still allow specific IP addresses from that country.\n\nWhich combination of steps should the Architect implement to satisfy this requirement? (Select TWO.)",
        "options": [
            {
                "isCorrect": true,
                "text": "​\n\nUsing AWS WAF, create a web ACL with a rule that explicitly allows requests from approved IP addresses declared in an IP Set.\n\n(Correct)"
            },
            {
                "isCorrect": true,
                "text": "​\n\nAdd another rule in the AWS WAF web ACL with a geo match condition that blocks requests that originate from a specific country.\n\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nSet up a geo match condition in the Application Load Balancer that blocks requests from a specific country."
            },
            {
                "isCorrect": false,
                "text": "​\n\nPlace a Transit Gateway in front of the VPC where the application is hosted and set up Network ACLs that block requests that originate from a specific country."
            },
            {
                "isCorrect": false,
                "text": "​\n\nIn the Application Load Balancer, create a listener rule that explicitly allows requests from approved IP addresses."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>If you want to allow or block web requests based on the country that the requests originate from, create one or more geo match conditions. A geo match condition lists countries that your requests originate from. Later in the process, when you create a web ACL, you specify whether to allow or block requests from those countries.</p><p>You can use geo match conditions with other AWS WAF Classic conditions or rules to build sophisticated filtering. For example, if you want to block certain countries but still allow specific IP addresses from that country, you could create a rule containing a geo match condition and an IP match condition. Configure the rule to block requests that originate from that country and do not match the approved IP addresses. As another example, if you want to prioritize resources for users in a particular country, you could include a geo match condition in two different rate-based rules. Set a higher rate limit for users in the preferred country and set a lower rate limit for all other users.</p><p><img src=\"https://udemy-images.s3.amazonaws.com/redactor/raw/2020-05-18_02-02-15-d61e11a7ef49c4e090a2d3a70b524779.png\"></p><p>If you are using the CloudFront geo restriction feature to block a country from accessing your content, any request from that country is blocked and is not forwarded to AWS WAF Classic. So if you want to allow or block requests based on geography plus other AWS WAF Classic conditions, you should <em>not</em> use the CloudFront geo restriction feature. Instead, you should use an AWS WAF Classic geo match condition.</p><p>Hence, the correct answers are:</p><p><strong>Using AWS WAF, create a web ACL with a rule that explicitly allows requests from approved IP addresses declared in an IP Set.</strong></p><p><strong>Add another rule in the AWS WAF web ACL with a geo match condition that blocks requests that originate from a specific country.</strong></p><p>The option that says: <strong>In the Application Load Balancer, create a listener rule that explicitly allows requests from approved IP addresses</strong> is incorrect because a listener rule just checks for connection requests using the protocol and port that you configure. It only determines how the load balancer routes the requests to its registered targets.</p><p>The option that says: <strong>Set up a geo match condition in the Application Load Balancer that block requests that originate from a specific country</strong> is incorrect because you can't configure a geo match condition in an Application Load Balancer. You have to use AWS WAF instead.</p><p>The option that says: <strong>Place a Transit Gateway in front of the VPC where the application is hosted and set up Network ACLs that block requests that originate from a specific country</strong> is incorrect because AWS Transit Gateway is simply a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. Using this type of gateway is not warranted in this scenario. Moreover, Network ACLs are not suitable for blocking requests from a specific country. You have to use AWS WAF instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-geo-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-geo-conditions.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/how-aws-waf-works.html\">https://docs.aws.amazon.com/waf/latest/developerguide/how-aws-waf-works.html</a></p><p><br></p><p><strong>Check out this AWS WAF Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p></div>"
    },
    {
        "questionNo": 58,
        "questionText": "You are working for a multinational telecommunications company. Your IT Manager is willing to consolidate their log streams including the access, application, and security logs in one single system. Once consolidated, the company wants to analyze these logs in real-time based on heuristics. There will be some time in the future where the company will need to validate heuristics, which requires going back to data samples extracted from the last 12 hours. \n\nWhat is the best approach to meet this requirement?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nFirst, send all the log events to Amazon SQS then set up an Auto Scaling group of EC2 servers to consume the logs and finally, apply the heuristics."
            },
            {
                "isCorrect": false,
                "text": "​\n\nFirst, set up an Auto Scaling group of EC2 servers then store the logs on Amazon S3 then finally, use EMR to apply heuristics on the logs."
            },
            {
                "isCorrect": true,
                "text": "​\nFirst, send all of the log events to Amazon Kinesis then afterwards, develop a client process to apply heuristics on the logs.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nFirst, configure Amazon Cloud Trail to receive custom logs and then use EMR to apply heuristics on the logs."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, you need a service that can collect, process, and analyze data in real-time hence, the right service to use here is Amazon Kinesis.</p><p>Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.</p><p>With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.</p><p>All other options are incorrect since these services do not have real-time processing capability, unlike Amazon Kinesis.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p></div>"
    },
    {
        "questionNo": 59,
        "questionText": "You are running an m5ad.large EC2 instance with a default attached 75 GB SSD instance-store backed volume. You shut it down and then start the instance. You noticed that the data which you have saved earlier on the attached volume is no longer available.\n\nWhat might be the cause of this?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nThe EC2 instance was using EBS backed root volumes, which are ephemeral and only live for the life of the instance."
            },
            {
                "isCorrect": false,
                "text": "​\nThe volume of the instance was not big enough to handle all of the processing data."
            },
            {
                "isCorrect": false,
                "text": "​\nThe instance was hit by a virus that wipes out all data."
            },
            {
                "isCorrect": true,
                "text": "​\nThe EC2 instance was using instance store volumes, which are ephemeral and only live for the life of the instance.\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>An <em>instance store</em> provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p><p>An instance store consists of one or more instance store volumes exposed as block devices. The size of an instance store as well as the number of devices available varies by instance type. While an instance store is dedicated to a particular instance, the disk subsystem is shared among instances on a host computer.</p><p>The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists. However, data in the instance store is lost under the following circumstances:</p><p>- The underlying disk drive fails</p><p>- The instance stops</p><p>- The instance terminates</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
    },
    {
        "questionNo": 60,
        "questionText": "You are working for a data analytics startup that collects clickstream data and stores them in an S3 bucket. You need to launch an AWS Lambda function to trigger your ETL jobs to run as soon as new data becomes available in Amazon S3.   \n\nWhich of the following services can you use as an extract, transform, and load (ETL) service in this scenario?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nRedshift Spectrum"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAWS Step Functions"
            },
            {
                "isCorrect": false,
                "text": "​\n\nS3 Select"
            },
            {
                "isCorrect": true,
                "text": "​\n\nAWS Glue\n\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You can create and run an ETL job with a few clicks in the AWS Management Console. You simply point AWS Glue to your data stored on AWS, and AWS Glue discovers your data and stores the associated metadata (e.g. table definition and schema) in the AWS Glue Data Catalog. Once cataloged, your data is immediately searchable, queryable, and available for ETL. AWS Glue generates the code to execute your data transformations and data loading processes.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p><p><br></p><p><strong>Introduction to AWS Glue:</strong></p><p><a href=\"https://youtu.be/qgWMfNSN9f4\">https://youtu.be/qgWMfNSN9f4</a></p></div>"
    },
    {
        "questionNo": 61,
        "questionText": "A financial analytics application that collects, processes and analyzes stock data in real-time is using Kinesis Data Streams. The producers continually push data to Kinesis Data Streams while the consumers process the data in real time. In Amazon Kinesis, where can the consumers store their results? (Select TWO.)",
        "options": [
            {
                "isCorrect": true,
                "text": "​\nAmazon Redshift\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nGlacier Select"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAmazon Athena"
            },
            {
                "isCorrect": true,
                "text": "​\nAmazon S3\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\n\nAWS Glue"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In Amazon Kinesis, the producers continually push data to Kinesis Data Streams and the consumers process the data in real time. Consumers (<em>such as a custom application running on Amazon EC2, or an Amazon Kinesis Data Firehose delivery stream</em>) can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.</p><p>Hence, <strong>Amazon S3</strong> and <strong>Amazon Redshift</strong> are the correct answers. The following diagram illustrates the high-level architecture of Kinesis Data Streams:</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\"></p><p><br></p><p><strong>Glacier Select</strong> is incorrect because this is not a storage service. It is primarily used to run queries directly on data stored in Amazon Glacier, retrieving only the data you need out of your archives to use for analytics.</p><p><strong>AWS Glue</strong> is incorrect because this is not a storage service. It is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.</p><p><strong>Amazon Athena</strong> is incorrect because this is just an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. It is not a storage service where you can store the results processed by the consumers.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"http://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">http://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p></div>"
    },
    {
        "questionNo": 62,
        "questionText": "The start-up company that you are working for has a batch job application that is currently hosted on an EC2 instance. It is set to process messages from a queue created in SQS with default settings. You configured the application to process the messages once a week. After 2 weeks, you noticed that not all messages are being processed by the application.\n\nWhat is the root cause of this issue?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\nMissing permissions in SQS."
            },
            {
                "isCorrect": true,
                "text": "​\nAmazon SQS has automatically deleted the messages that have been in a queue for more than the maximum message retention period.\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nThe batch job application is configured to long polling."
            },
            {
                "isCorrect": false,
                "text": "​\nThe SQS queue is set to short-polling."
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days. Since the queue is configured to the default settings and the batch job application only processes the messages once a week, the messages that are in the queue for more than 4 days are deleted. This is the root cause of the issue.</p> <p>To fix this, you can increase the message retention period to a maximum of 14 days using the&nbsp;<a href=\"http://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html\">SetQueueAttributes</a>&nbsp;action.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p> <p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-message-lifecycle.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-message-lifecycle.html</a></p> <p>&nbsp;&nbsp;&nbsp;&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/\"><span>https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p></div>"
    },
    {
        "questionNo": 63,
        "questionText": "You recently launched a news website which is expected to be visited by millions of people around the world. You chose to deploy the website in AWS to take advantage of its extensive range of cloud services and global infrastructure. Aside from AWS Region and Availability Zones, which of the following is part of the AWS Global Infrastructure that is used for content distribution?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nVPC Endpoint"
            },
            {
                "isCorrect": false,
                "text": "​\n\nHypervisor"
            },
            {
                "isCorrect": true,
                "text": "​\nEdge Location\n(Correct)"
            },
            {
                "isCorrect": false,
                "text": "​\nBastion Hosts"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>An edge location helps deliver high availability, scalability, and performance of your application for all of your customers from anywhere in the world. This is used by other services such as Lambda and Amazon CloudFront.</p><p>Amazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. CloudFront delivers your files to end-users using a global network of edge locations.</p><p><strong>Bastion Hosts</strong> is incorrect because a bastion host is not part of the AWS Global Infrastructure. It is just a host computer or a \"jump server\" used to allow SSH access to your EC2 instances from an outside network.</p><p><strong>Hypervisor</strong> is incorrect because this is just a computer software, firmware or hardware that creates and runs virtual machines. This technology relates to EC2 instances but it is not part of the AWS Global Infrastructure.</p><p><strong>VPC Endpoint</strong> is incorrect because this is not part of the AWS Global Infrastructure and is just used to privately connect your VPC to other AWS services and endpoint services.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><a href=\"https://aws.amazon.com/about-aws/global-infrastructure/\">https://aws.amazon.com/about-aws/global-infrastructure/</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/\">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</a></p></div>"
    },
    {
        "questionNo": 64,
        "questionText": "You are implementing a hybrid architecture for your company where you are connecting their Amazon Virtual Private Cloud (VPC) to their on-premises network. Which of the following can be used to create a private connection between the VPC and your company's on-premises network?",
        "options": [
            {
                "isCorrect": false,
                "text": "​\n\nRoute 53"
            },
            {
                "isCorrect": false,
                "text": "​\nClassicLink"
            },
            {
                "isCorrect": false,
                "text": "​\nAWS Direct Link"
            },
            {
                "isCorrect": true,
                "text": "​\nDirect Connect\n(Correct)"
            }
        ],
        "explain": "<h4>Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Direct Connect creates a direct, private connection from your on-premises data center to AWS, letting you establish a 1-gigabit or 10-gigabit dedicated network connection using Ethernet fiber-optic cable.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/connect-vpc/\">https://aws.amazon.com/premiumsupport/knowledge-center/connect-vpc/</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p><p><br></p><p><strong>Additional tutorial - how to configure a VPN over AWS Direct Connect:</strong></p><p><a href=\"https://youtu.be/dhpTTT6V1So\">https://youtu.be/dhpTTT6V1So</a></p><p><br></p><p><strong>S3 Transfer Acceleration vs Direct Connect vs VPN vs Snowball vs Snowmobile:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/\">https://tutorialsdojo.com/aws-cheat-sheet-s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/\">https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/</a></p></div>"
    }
]